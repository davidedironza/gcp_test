{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_mode:  test\n",
      "0.4\n",
      "try:  2019-05-23 22:00:00+02:00\n",
      "2019-05-23 22:00:00+02:00\n",
      "projects new:  0\n",
      "no valid building projects data found\n",
      "6.1\n",
      "hier normal Aufruf tranche.report_exception(traceback.format_exc()) - im GCP PoC nicht\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-28-5074dc6b5d53>\", line 238, in <module>\n",
      "    raise Exception\n",
      "Exception\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "**************************************************************************************\n",
    "* Script-Purpose : Selection of Bindexis Triggers \n",
    "* source         : Bindexis Extration run every day - getting the external base data\n",
    "* target         : Hybris CRM trigger system - every day after fetching base data\n",
    "* running context: development mode but stage == ACC (because of target system Hybris)\n",
    "* authors        : Davide di Ronza, Gerhard Pachl\n",
    "* creation date  : 2019-04-24\n",
    "* last change    : 2019-05-23\n",
    "* description    : program transforms external base data from bindexis to potential \n",
    "                   triggers for Hybris CRM Marketing\n",
    "                   external data will be matched to our internal customer data\n",
    "                   to identify and match our customers\n",
    "                   and to create a lead to contact them for potential new business\n",
    "**************************************************************************************\n",
    "\"\"\"\n",
    "\n",
    "try: \n",
    "    #0 Modul-Import & Parametrierung    \n",
    "    #0.1 Modul-Import\n",
    "    \n",
    "    # important if test or prod_mode\n",
    "    run_mode = \"go_hybris\"\n",
    "    run_mode = \"test\"\n",
    "    print(\"run_mode: \", run_mode)\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(location=\"europe-west6\")\n",
    "    from google.cloud import storage\n",
    "     \n",
    "    # change these to try this notebook out\n",
    "    BUCKET = 'axa-ch-raw-dev-dla'\n",
    "    PROJECT = 'axa-ch-datalake-analytics-dev'\n",
    "    REGION = 'eu-west6'\n",
    "        \n",
    "    import re\n",
    "    import random\n",
    "    import locale\n",
    "    import traceback\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import datetime\n",
    "    import pytz\n",
    "    import time\n",
    "    import os\n",
    "    import importlib\n",
    "    import tempfile\n",
    "    import sys\n",
    "    sys.path.insert(0, './functions')\n",
    "    \n",
    "    # standard python scripts as libraries with additional necessary functions\n",
    "    import auxiliary_gc as aux\n",
    "    importlib.reload(aux)\n",
    "    \n",
    "    import campaign_management_gc as cm\n",
    "    importlib.reload(cm)\n",
    "  \n",
    "    #import functions.soa_gc as soa\n",
    "    import soa_gc as soa\n",
    "    importlib.reload(soa)\n",
    " \n",
    "\n",
    "    #0.2 path information to retrieve bucket data from gcp storage\n",
    "    path_data_va = \"bindexis/data/various/\"\n",
    "    path_data_input = \"bindexis/data/input/\"\n",
    "    \n",
    "    #path_data = 'gs://axa-ch-raw-dev-dla/bindexis/data/various/kampagne.pkl' \n",
    "    path_data = 'bindexis/data/various/kampagne.pkl'\n",
    "        \n",
    "    #0.3 Set timezone\n",
    "    os.environ['TZ'] = 'Europe/Zurich'\n",
    "    time.tzset()\n",
    "    \n",
    "    # retrieve Bindexis Data\n",
    "    #0.4 Access & Authentication Setup\n",
    "    print('0.4')\n",
    "    time_now = datetime.datetime.now(pytz.timezone('Europe/Zurich'))\n",
    "\n",
    "    # function for setting expiration date of a table - mainly fpr temporary tables\n",
    "    def tmp_table_expiration(table_ref_in, minutes_in):\n",
    "        table = client.get_table(table_ref_in) \n",
    "    #assert table.expires is None\n",
    "        expiration = datetime.datetime.now(pytz.utc) + datetime.timedelta(minutes=minutes_in)\n",
    "        table.expires = expiration\n",
    "        table_ref = client.update_table(table, [\"expires\"])  # API request\n",
    "    \n",
    "    # function needed for check if created table already exists \n",
    "    def bq_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False \n",
    "    \n",
    "    # necessary parameters for checking if bq temporary table exists\n",
    "    project_nm = 'axa-ch-datalake-analytics-dev'\n",
    "    dataset_nm = 'temp_da'\n",
    "    table_nm = 'tmp_bau_projects'\n",
    "    \n",
    "    # declare the BigQuery Python object for table queries\n",
    "    client = bigquery.Client(project=project_nm,location=\"europe-west6\")   \n",
    "\n",
    "    dataset = client.dataset(dataset_nm)\n",
    "    table_ref = dataset.table(table_nm)    \n",
    "    \n",
    "    # set stage level - it is important for AXA SAP Systems like \"DEV\", \"ACC\", \"PROD\"\n",
    "    # org stage = sf.platform_is_server(\"stage\")\n",
    "    stage = 'ACC'\n",
    "    locale.getlocale()\n",
    "    # locale.setlocale(locale.LC_ALL, \"German\") \n",
    "    \n",
    "    # read latest trigger run timestamp from pickle file from bucket\n",
    "    client_cs = storage.Client()\n",
    "    bucket_cs = client_cs.get_bucket(BUCKET)\n",
    "    \n",
    "    # read latest date  trigger run and successful delivery to hybris\n",
    "    try:\n",
    "        filename = \"{}/kampagne.pkl\".format(tempfile.gettempdir())\n",
    "        blob = bucket_cs.blob(path_data_va+'kampagne.pkl')\n",
    "        blob.download_to_filename(filename)\n",
    "        with open(filename, 'rb') as fp: campaign_timelastrun = pickle.load(fp)\n",
    "        print(\"try: \" , campaign_timelastrun)\n",
    "    except:\n",
    "        campaign_timelastrun      = time_now + datetime.timedelta(days=-1)\n",
    "        print(\"except: \" , campaign_timelastrun)    \n",
    "   \n",
    "    print(campaign_timelastrun)\n",
    "    \n",
    "    # get plz for main agencies - the original csv. file is stored in a bucket\n",
    "    # but we löaded the data into a BQ table\n",
    "    # plz_df = pd.read_csv(r'gs://axa-ch-raw-dev-dla/bindexis/data/various/plz_dict.csv', sep=';', header =[0] )\n",
    "    \n",
    "    sql_statement = \"\"\"SELECT * FROM `axa-ch-datalake-analytics-dev.various_da.va_ga_plz`\"\"\"\n",
    "    plz_df = client.query(sql_statement).to_dataframe().drop_duplicates()\n",
    "    #plz_df.head()\n",
    "    \n",
    "    # following sql originallly concept from tdb\n",
    "    # will not run because of special sql oracle functions - not availbale in standard sql - see next\n",
    "    # here change some statemtens for new va_dev_ad_de_v and cr_aktpol_v\n",
    "    # sql_statement = \"\"\"\n",
    "    #    select de_id, stats_mode(de_name_kurz) as GA_NAME, stats_mode(sprache_cdi) as GA_SPRACHE, \n",
    "    #    stats_mode(de_typ_cddev) as de_typ_cddev\n",
    "    #    from `axa-ch-datalake-analytics-dev.various.va_dev_ad_de_v`\n",
    "    #    where de_id in (select distinct org_nl_ga_b\n",
    "    #                    from `axa-ch-datalake-analytics-dev.contract.cr_aktpol_v`\n",
    "    #                    from tdbmk.agr_aktpol_v\n",
    "    #                    where cor_stichtag_yyyymm = (select max(cor_stichtag_yyyymm) \n",
    "    #                    from `axa-ch-datalake-analytics-dev.contract.cr_aktpol_v`)\n",
    "    #                    and org_nlel_kanal_b in ('AD', 'DIREKT'))\n",
    "    #    group by de_id\"\"\"\n",
    "    \n",
    "    # df_dict = sf.sql_getdf(sql_statement, con_tdb, column_lower = False)\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    with dev_ad_de as \n",
    "    (select de_id, de_name_kurz as GA_NAME, sprache_cdi as GA_SPRACHE, de_typ_cddev, count(de_id) AS counts\n",
    "     from `axa-ch-data-engineering-dev.various.va_dev_ad_de`\n",
    "     where de_id in (select distinct GA_NL_B\n",
    "                           from `axa-ch-data-engineering-dev.contract.cr_aktpol_m`\n",
    "                           where stichtag = (select max(stichtag) from `axa-ch-data-engineering-dev.contract.cr_aktpol_m`)\n",
    "                           and kanal_b in ('AD', 'DIREKT'))\n",
    "     group by de_id, ga_name, ga_sprache, de_typ_cddev)\n",
    "     , ranked as (select de_id, ga_name, ga_sprache, de_typ_cddev, \n",
    "     ROW_NUMBER() OVER (PARTITION BY de_id ORDER BY counts DESC) rank from dev_ad_de)\n",
    "     select DE_ID, GA_NAME, GA_SPRACHE, DE_TYP_CDDEV from ranked where rank = 1\n",
    "     \"\"\"\n",
    "    # change by original va_dev_ad_de\n",
    "    \n",
    "    df_dict = client.query(sql).to_dataframe().drop_duplicates()\n",
    "    \n",
    "    # df_dict = sf.sql_getdf(sql_statement, con_tdb, column_lower = False)\n",
    "     \n",
    "    df_dict = df_dict.rename(columns = {'DE_ID': 'GA_ID'}).set_index('GA_ID')\n",
    "    df_dict = df_dict[['GA_SPRACHE']]\n",
    "    \n",
    "    ga_dict = df_dict.to_dict('index')\n",
    "    for key, value in ga_dict.items():\n",
    "        ga_dict[key] = value.get('GA_SPRACHE')\n",
    "        \n",
    "    prospect_sharekg = 0.05\n",
    "    #ga_dict\n",
    "#except:\n",
    "#    print(\"error\") \n",
    "    \n",
    "    # 20190429*gep initialising campaign object (in standard function lib cm)\n",
    "    # 0.5 Initialisierung Kampagnen-Objekt\n",
    "    campaign = cm.Campaign(campaign_id              = 80017,\n",
    "                           campaign_name            = \"Bindexis Bauausschreibungen\", \n",
    "                           campaign_manager         = [\"thomas.knell@axa-winterthur.ch\"],\n",
    "                           campaign_techsupport     = [\"tobias.ippisch@axa-winterthur.ch\",\n",
    "                                                       \"natascha.spindler@axa-winterthur.ch\",\n",
    "                                                       \"IMCEASMS-0041799422212@sms.wgr\"],                  \n",
    "                           campaign_sharekg         = \"Permanent\",\n",
    "                           campaign_channelsplit    = {\"AD\": 1.0},                                                       \n",
    "                           campaign_channelsplitvar = None,               \n",
    "                           campaign_startdate       = \"16.10.2017\",\n",
    "                           campaign_enddate         = \"31.12.2025\",\n",
    "                           campaign_lineofbusiness  = \"NL\",\n",
    "                           campaign_pathdata        = path_data, \n",
    "                           campaign_trackausschluss = True)                           \n",
    "\n",
    "    #1   Pulling  Raw data \n",
    "    #1.1 Pulling base data bindexis (projects, contacts, buildings)\n",
    "    #print(\"1.1\")\n",
    "    \n",
    "    # sql_statement = \"\"\"\n",
    "    #         select *\n",
    "    #         from fbtdbmk.bindexis_bau_projects\n",
    "    #         where PROJECT_INRESEARCH = 0\n",
    "    #         and DATE_INSERTION > to_date('{0}', 'dd.mm.yyyy')\"\"\".format(campaign.campaign_timelastrun.strftime(\"%d.%m.%Y\"))\n",
    "                   \n",
    "    #     df_projects = sf.sql_getdf(sql_statement, con_tdb, column_lower = False)  \n",
    "\n",
    "    # very important if \"test\" then selecting always data of the day otherwise only once a day\n",
    "                           \n",
    "    if run_mode == \"test\": \n",
    "        cond_str = \"DATE_INSERTION\"\n",
    "    else:\n",
    "        cond_str = \"date(DATE_INSERTION)\" \n",
    "        \n",
    "    sql = \"\"\"SELECT * FROM `axa-ch-datalake-analytics-dev.BINDEXIS.bindexis_bau_projects2`\n",
    "                 where PROJECT_INRESEARCH = false\n",
    "                 and ADDRESS_COUNTRY = \"CH\" \n",
    "                 and {0} > '{1}' \"\"\".format(cond_str, campaign_timelastrun.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    df_projects = client.query(sql).to_dataframe()\n",
    "    df_projects[[\"PROJECT_ID\"]].drop_duplicates()\n",
    "#    print(df_projects.head())\n",
    "                           \n",
    "    print(\"projects new: \", df_projects.PROJECT_ID.count())\n",
    "\n",
    "    # if no data - then exit program here\n",
    "    if df_projects.PROJECT_ID.count() == 0:\n",
    "        print(\"no valid building projects data found\")\n",
    "        raise Exception\n",
    "    \n",
    "    # if new projects data available create new temporary table with projects data for further use\n",
    "    \n",
    "    sql = \"\"\"CREATE OR REPLACE TABLE `axa-ch-datalake-analytics-dev.temp_da.tmp_bau_projects`\n",
    "             OPTIONS( expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 120 MINUTE)) AS\n",
    "             SELECT distinct PROJECT_ID FROM `axa-ch-datalake-analytics-dev.BINDEXIS.bindexis_bau_projects2`\n",
    "                    where PROJECT_INRESEARCH = false and ADDRESS_COUNTRY = \"CH\"\n",
    "                    and {0} > '{1}' \"\"\".format(cond_str, campaign_timelastrun.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    # if table does not exist please wait until it is created\n",
    "    query_job = client.query(sql)\n",
    "    query_job.result()  # waits for query to finish job.result()\n",
    "    \n",
    "    contact_dict = {\"CONTACT_TYPE\":         \"ORG_TYPE\",\n",
    "                    \"CONTACT_ID\":           \"ORG_ID\",  \n",
    "                    \"CONTACT_GENDER\":       \"PERSON_GENDER\", \n",
    "                    \"CONTACT_FIRSTNAME\":    \"PERSON_FIRSTNAME\", \n",
    "                    \"CONTACT_LASTNAME\":     \"PERSON_LASTNAME\", \n",
    "                    \"CONTACT_ORGANIZATION\": \"ORG_NAME\", \n",
    "                    \"CONTACT_STREET1\":      \"ORG_STREET1\", \n",
    "                    \"CONTACT_STREET2\":      \"ORG_STREET2\",              \n",
    "                    \"CONTACT_STREET3\":      \"ORG_STREET3\",              \n",
    "                    \"CONTACT_POSTALCODE\":   \"ORG_POSTALCODE\",  \n",
    "                    \"CONTACT_CITY\":         \"ORG_CITY\",  \n",
    "                    \"CONTACT_COUNTRY\":      \"ORG_COUNTRY\",\n",
    "                    \"CONTACT_PHONE1\":       \"ORG_PHONE\",\n",
    "                    \"CONTACT_PHONE2\":       \"PERSON_PHONE\",\n",
    "                    \"CONTACT_PHONE3\":       \"PERSON_MOBILE\",\n",
    "                    \"CONTACT_EMAIL1\":       \"ORG_EMAIL\",\n",
    "                    \"CONTACT_EMAIL2\":       \"PERSON_EMAIL\",\n",
    "                    \"CONTACT_WEB\":          \"ORG_WEB\",\n",
    "                    \"CONTACT_PERSON_ID\":    \"PERSON_ID\"}\n",
    "    \n",
    "    # 20190429*gep - hier weiter mit join auf df_projects auf df_buildings\n",
    "    \n",
    "    # df_contacts = sf.sql_leftjoin(df = df_projects[[\"PROJECT_ID\"]].drop_duplicates(), connection = con_tdb,\n",
    "    #                 schema = \"fbtdbmk\", tablename = \"bindexis_bau_contacts\", key = \"PROJECT_ID\",\n",
    "    #                 attributes = contact_dict.values(), \n",
    "    #                 column_lower = False) \n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT \n",
    "        a.PROJECT_ID, \n",
    "        b.ORG_ID, b.PERSON_GENDER, b.PERSON_FIRSTNAME, b.PERSON_LASTNAME, b.ORG_NAME, b.ORG_TYPE, b.ORG_STREET1, b.ORG_STREET2, \n",
    "        b.ORG_STREET3, b.ORG_POSTALCODE, b.ORG_CITY, b.ORG_COUNTRY, b.ORG_PHONE, b.PERSON_PHONE, b.PERSON_MOBILE, b.ORG_EMAIL, \n",
    "        b.PERSON_EMAIL, b.ORG_WEB, b.PERSON_ID\n",
    "        FROM       `axa-ch-datalake-analytics-dev.temp_da.tmp_bau_projects` a\n",
    "        left join `axa-ch-datalake-analytics-dev.BINDEXIS.bindexis_bau_contacts2` b\n",
    "        on a.PROJECT_ID = b.PROJECT_ID\n",
    "    \"\"\"\n",
    "    \n",
    "    df_contacts = client.query(sql).to_dataframe()\n",
    "    \n",
    "    sql = \"\"\"SELECT a.PROJECT_ID, b.BUILDING_TYPE, b.BUILDING_DEVELOPMENT \n",
    "        FROM       `axa-ch-datalake-analytics-dev.temp_da.tmp_bau_projects` a    \n",
    "        LEFT JOIN  `axa-ch-datalake-analytics-dev.BINDEXIS.bindexis_bau_buildings2` b\n",
    "        on a.PROJECT_ID = b.PROJECT_ID\"\"\"\n",
    "     \n",
    "    # change by `axa-ch-datalake-analytics-dev.BINDEXIS.bindexis_bau_buildings`\n",
    "    \n",
    "    df_buildings = client.query(sql).to_dataframe()\n",
    "\n",
    "    \n",
    "    #1.2 Aufbereitung Projektdaten\n",
    "    print(\"1.2\")\n",
    "    aux_dict = {\"PROJECT_TITLE\":      \"\",   \"PROJECT_DESCRIPTION\": \"\",   \"PROJECT_VALUE\":    0,   \n",
    "                \"PROJECT_APARTMENTS\":  0,   \"PROJECT_PARCELID\":    \"\",   \"ADDRESS_STREET1\": \"\", \n",
    "                \"ADDRESS_STREET2\":    \"\",   \"ADDRESS_STREET3\":     \"\",   \"ADDRESS_CITY\":    \"\"}       \n",
    "        \n",
    "    for i in aux_dict.keys(): df_projects[i] = df_projects[i].fillna(aux_dict[i])\n",
    "        \n",
    "    #df_projects.head()\n",
    "    \n",
    "    #1.3 Aufbereitung Kontaktdaten\n",
    "    print(\"1.3\")\n",
    "    #Gruppierung Umformatierung Daten + Spaltenbereinigung\n",
    "    for i in contact_dict.keys(): df_contacts[i] = df_contacts[contact_dict[i]].fillna(\"\")\n",
    "        \n",
    "    aux_list = [i for i in contact_dict.keys()] + [\"PROJECT_ID\"]\n",
    "    df_contacts = df_contacts[aux_list].copy()\n",
    "    # df_contacts.head()\n",
    "    \n",
    "    #Klassifikation P- & U-Kontakte    \n",
    "    def check_commercial_contact(firstname, lastname, orgname, orgtype, personid, contactid):        \n",
    "        if (firstname == \"\") & (lastname == \"\"): \n",
    "            return (orgname, \"U\", contactid) #Organisationskontakt ohne persönlichen Ansprachpartner\n",
    "        elif (firstname in orgname) & (lastname in orgname) & (orgtype in [\"Bauherr\", \"Bauherrenvertreter\"]): \n",
    "            return (\"\", \"P\", personid) #Privatpersonen als Organisation benannt (Bauherr, Bauherrenvertreter)\n",
    "        elif (firstname in orgname) & (lastname in orgname) & (orgtype not in [\"Bauherr\", \"Bauherrenvertreter\"]): \n",
    "            return (\"\", \"U\", personid) #Privatpersonen als Organisation benannt (nicht Bauherr, Bauherrenvertreter)        \n",
    "        else:\n",
    "            return (orgname, \"U\", contactid) #Organisationskontakt mit persönlichem Ansprechpartner\n",
    "        \n",
    "    df_contacts[\"AUX\"] = df_contacts.apply(lambda row: check_commercial_contact( \\\n",
    "                         row.CONTACT_FIRSTNAME, row.CONTACT_LASTNAME, row.CONTACT_ORGANIZATION, \n",
    "                         row.CONTACT_TYPE, row.CONTACT_PERSON_ID, row.CONTACT_ID), axis = 1)\n",
    "    \n",
    "    df_contacts[\"CONTACT_ORGANIZATION\"] = df_contacts.AUX.apply(lambda x: x[0])         \n",
    "    df_contacts[\"CONTACT_ORGTYPE\"]      = df_contacts.AUX.apply(lambda x: x[1])    \n",
    "    df_contacts[\"CONTACT_ID\"]           = df_contacts.AUX.apply(lambda x: x[2])  \n",
    "    df_contacts[\"CONTACT_ORGTYPE_NUM\"]  = df_contacts.CONTACT_ORGTYPE.map({\"P\": 1, \"U\": 2})     \n",
    "    \n",
    "    #Zusammenführung Telefon- & Email-Adressen\n",
    "    df_contacts[\"CONTACT_EMAIL1\"] = df_contacts.apply(lambda row: row.CONTACT_EMAIL2 if row.CONTACT_EMAIL1 == \"\" else row.CONTACT_EMAIL1, axis = 1)\n",
    "    df_contacts[\"CONTACT_EMAIL2\"] = df_contacts.apply(lambda row: \"\" if row.CONTACT_EMAIL1 == row.CONTACT_EMAIL2 else row.CONTACT_EMAIL2, axis = 1)\n",
    "        \n",
    "    def tel_collector(tel1, tel2, tel3):\n",
    "        tel_list = []\n",
    "        for i in [tel1, tel2, tel3]:\n",
    "            if i != \"\": tel_list += [i]\n",
    "        return tel_list\n",
    "        \n",
    "    df_contacts[\"AUX\"] = df_contacts.apply(lambda row: tel_collector(row[\"CONTACT_PHONE1\"], row[\"CONTACT_PHONE2\"], row[\"CONTACT_PHONE3\"]), axis = 1)\n",
    "    \n",
    "    for i in range(3): \n",
    "        df_contacts[\"CONTACT_PHONE\" + str(i+1)] = df_contacts.AUX.apply(lambda x: x[i] if len(x) >= i+1 else \"\")\n",
    "    \n",
    "    #1.4 Aufbereitung Objektdaten\n",
    "    print(\"1.4\")\n",
    "    #Aufbereitung Development Type (Dummyfizierung)\n",
    "    development_list = [\"Neubau\", \"Umbau\", \"Anbau\", \"Abbruch\", \"Äussere Veränderungen\"]\n",
    "    \n",
    "    aux_df = pd.DataFrame(df_buildings.groupby(\"PROJECT_ID\").BUILDING_DEVELOPMENT.unique())\n",
    "    \n",
    "    for i in development_list:\n",
    "        aux_df[\"BUILDING_DEVELOPMENT_\" + str(i).replace(\" \", \"_\").upper()] = aux_df.BUILDING_DEVELOPMENT.apply(lambda x: 1 if i in x else 0)\n",
    "        \n",
    "    df_buildings = df_buildings.merge(aux_df.drop(\"BUILDING_DEVELOPMENT\", axis = 1), how = \"left\", left_on = \"PROJECT_ID\", right_index = True)\n",
    "    df_buildings = df_buildings.drop(\"BUILDING_DEVELOPMENT\", axis = 1)\n",
    "    \n",
    "    # df_buildings.head()\n",
    "    \n",
    "    #Aufbereitung Building Type allgemein (Dummyfizierung)\n",
    "    type_list = [\"Verkehrsanlagen\", \"Freizeit, Sport, Erholung\", \"Wohnen (bis 2 Wohneinheiten)\",\n",
    "                 \"Wohnen (ab 3 Wohneinheiten)\",  \"Technische Anlagen\", \"Industrie und Gewerbe\",\n",
    "                  \"Land- und Forstwirtschaft\", \"Unterricht, Bildung und Forschung\",\n",
    "                  \"Kultur und Geselligkeit\", \"Gastgewerbe und Fremdenverkehr\", \"Handel und Verwaltung\", \n",
    "                  \"Kultus\", \"Fürsorge und Gesundheit\", \"Militär- und Schutzanlagen\", \n",
    "                  \"Justiz und Polizei\"]\n",
    "    \n",
    "    aux_df = df_buildings[df_buildings.BUILDING_TYPE.notnull()][[\"PROJECT_ID\", \"BUILDING_TYPE\"]].copy()\n",
    "        \n",
    "    for i in type_list:\n",
    "        aux_str = re.sub('[()-, ]', '', str(i)).upper()\n",
    "        aux_df[\"BUILDING_TYPE_\" + aux_str] = aux_df.BUILDING_TYPE.apply(lambda x: 1 if i in x else 0)\n",
    "        aux_df[\"BUILDING_TYPE\"] = aux_df.BUILDING_TYPE.apply(lambda x: x.replace(i + \"-\", \"\"))\n",
    "        aux_df[\"BUILDING_TYPE_\" + aux_str] = aux_df.groupby(\"PROJECT_ID\")[\"BUILDING_TYPE_\" + aux_str].transform(\"max\") \n",
    "        \n",
    "    df_buildings = df_buildings.merge(aux_df.drop(\"BUILDING_TYPE\", axis = 1).drop_duplicates(\"PROJECT_ID\"), how = \"left\", on = \"PROJECT_ID\")\n",
    "    \n",
    "    #Aufbereitung Building Type spezifisch (Concatenation)   \n",
    "    aux_df = pd.DataFrame(aux_df.groupby(\"PROJECT_ID\")[\"BUILDING_TYPE\"].apply(lambda x: \" / \".join(set(x))))\n",
    "        \n",
    "    df_buildings = df_buildings.drop(\"BUILDING_TYPE\", axis = 1).merge(aux_df, how = \"left\", left_on = \"PROJECT_ID\", right_index = True)\n",
    "        \n",
    "    #Deduplizierung & Feature Engineering\n",
    "    df_buildings = df_buildings.drop_duplicates(\"PROJECT_ID\").rename(columns = {\"BUILDING_TYPE\": \"BUILDING_DETAIL\"})\n",
    "    \n",
    "    #Befüllung Leerwerte\n",
    "    aux_list = [i for i in df_buildings.columns if \"BUILDING_DEVELOPMENT\" in i] + \\\n",
    "               [i for i in df_buildings.columns if \"BUILDING_TYPE\" in i]\n",
    "        \n",
    "    for i in aux_list: df_buildings[i].fillna(0, inplace = True)\n",
    "       \n",
    "    #Anschlüsselung an df_projects\n",
    "    df_projects = df_projects.merge(df_buildings, how = \"left\", on = \"PROJECT_ID\")\n",
    "    \n",
    "    #df_projects.head()\n",
    "    \n",
    "    # prepare df_contacts for later request as a temporary bigQuery table\n",
    "    \n",
    "    aux_list = [\"CONTACT_ID\",         \"CONTACT_EMAIL1\",       \"CONTACT_EMAIL2\", \n",
    "                \"CONTACT_PHONE1\",     \"CONTACT_PHONE2\",       \"CONTACT_PHONE3\",\n",
    "                \"CONTACT_FIRSTNAME\",  \"CONTACT_LASTNAME\",     \"CONTACT_STREET1\", \n",
    "                \"CONTACT_POSTALCODE\", \"CONTACT_ORGANIZATION\", \"CONTACT_ORGTYPE_NUM\"]\n",
    "    \n",
    "    df_contacts_2 = df_contacts[aux_list].drop_duplicates(\"CONTACT_ID\")\n",
    "    # df_contacts_2.head()\n",
    "    \n",
    "    # 2. loading a dataframe into new bq table (does not exist before), should be temporary for 10 minutes\n",
    "    dataset_ref = client.dataset('temp_da', project='axa-ch-datalake-analytics-dev')\n",
    "    table_ref = dataset_ref.table('tmp_contacts')\n",
    "    #expiration = datetime.datetime.now(pytz.utc) + datetime.timedelta(minutes=10)\n",
    "    #table_ref.expires = expiration #  seems to have an effect, but also no error when processing next step\n",
    "    \n",
    "    # client.load_table_from_dataframe(df_tmp_builds, table_ref, expiration, project='axa-ch-datalake-analytics-dev').result()\n",
    "    # result() waits until tables is loaded completely\n",
    "    client.load_table_from_dataframe(df_contacts_2, table_ref,  project='axa-ch-datalake-analytics-dev').result()\n",
    "    \n",
    "    # call function for setting expiration time otherwise never expires\n",
    "    # at creation step 2 --> qestion for google engineers\n",
    "    tmp_table_expiration(table_ref, 120)\n",
    "    \n",
    "    #1.5 Partner-Matching (exakt) \n",
    "    print(\"1.5\")    \n",
    "    sql = \"\"\"\n",
    "        select e.CONTACT_ID, d.PART_NR, 'EXAKT' as MATCH_TYPE, 100 as MATCH_SCORE, f.CONTACT_CUSTOMER\n",
    "        from \n",
    "            (select a.*, b.EMAIL_ADR_1, c.TEL_NR_KOMPL\n",
    "            from \n",
    "                    (select part_nr, gpart_typ_cdgpd, vname, nname, name_nnp_zeile1, plz,\n",
    "                     CONCAT(rtrim(ltrim(name_nnp_zeile1)),' ',rtrim(ltrim(name_nnp_zeile2))) AS name_nnp ,\n",
    "                     CONCAT(rtrim(ltrim( replace(STR_NAME, 'str.', 'strasse'))), ' ', rtrim(ltrim(haus_nr_kompl))) as strasse \n",
    "                     from `axa-ch-data-engineering-dev.customer.cs_partner`\n",
    "                     where ersi_dat = '9999-12-31'\n",
    "                     and kz_part_archvg_vgshn != '1') a \n",
    "                       \n",
    "            left outer join    \n",
    "                    (select part_nr, email_adr_1\n",
    "                     from `axa-ch-data-engineering-dev.customer.cs_email` \n",
    "                     where ersi_dat = '9999-12-31' and email_adr_1 is not NULL) b        \n",
    "                     on a.PART_NR = b.PART_NR \n",
    "                \n",
    "            left outer join    \n",
    "                    (select part_nr, tel_nr_kompl\n",
    "                     from `axa-ch-data-engineering-dev.customer.cs_telefon` \n",
    "                     where ersi_dat = '9999-12-31'\n",
    "                     and tel_nr_kompl is not NULL) c        \n",
    "                    on a.PART_NR = c.PART_NR ) d\n",
    "            \n",
    "        inner join\n",
    "            \n",
    "             `axa-ch-datalake-analytics-dev.temp_da.tmp_contacts` e  \n",
    "                \n",
    "        on (    d.TEL_NR_KOMPL in (e.CONTACT_PHONE1, e.CONTACT_PHONE2, e.CONTACT_PHONE3) \n",
    "                and d.GPART_TYP_CDGPD = cast(e.CONTACT_ORGTYPE_NUM as string))\n",
    "        or (    d.EMAIL_ADR_1  in (e.CONTACT_EMAIL1, e.CONTACT_EMAIL2)\n",
    "                and d.GPART_TYP_CDGPD = cast(e.CONTACT_ORGTYPE_NUM as string))\n",
    "        or (    e.CONTACT_FIRSTNAME    = d.VNAME\n",
    "                and e.CONTACT_LASTNAME     = d.NNAME\n",
    "                and e.CONTACT_STREET1      = d.STRASSE\n",
    "                and e.CONTACT_POSTALCODE   = d.PLZ\n",
    "                and cast(e.CONTACT_ORGTYPE_NUM as string)  = d.GPART_TYP_CDGPD)\n",
    "        or (    e.CONTACT_ORGANIZATION = d.NAME_NNP_ZEILE1\n",
    "                and e.CONTACT_STREET1      = d.STRASSE\n",
    "                and e.CONTACT_POSTALCODE   = d.PLZ                           \n",
    "                and cast(e.CONTACT_ORGTYPE_NUM as string)  = d.GPART_TYP_CDGPD)\n",
    "        or (    e.CONTACT_ORGANIZATION = d.NAME_NNP\n",
    "                and e.CONTACT_STREET1      = d.STRASSE\n",
    "                and e.CONTACT_POSTALCODE   = d.PLZ                           \n",
    "                and cast(e.CONTACT_ORGTYPE_NUM as string)  = d.GPART_TYP_CDGPD)\n",
    "            \n",
    "        left outer join\n",
    "                (select distinct PART_NR, 1 as CONTACT_CUSTOMER \n",
    "                 from `axa-ch-data-engineering-dev.contract.cr_aktpol_m`\n",
    "                 where stichtag = \n",
    "                 (select max(distinct stichtag) from `axa-ch-data-engineering-dev.contract.cr_aktpol_m`)) f\n",
    "    \n",
    "                 on d.PART_NR = f.PART_NR\"\"\"    \n",
    "        \n",
    "    df_temp1 = client.query(sql).to_dataframe()        \n",
    "    \n",
    "    # change by original cs_email, cs_partner, cs_telefon, cr_aktpol_m\n",
    "    \n",
    "    df_temp1.head()\n",
    "         \n",
    "    #aux_list = [\"CONTACT_ID\",         \"CONTACT_EMAIL1\",       \"CONTACT_EMAIL2\", \n",
    "    #            \"CONTACT_PHONE1\",     \"CONTACT_PHONE2\",       \"CONTACT_PHONE3\",\n",
    "    #            \"CONTACT_FIRSTNAME\",  \"CONTACT_LASTNAME\",     \"CONTACT_STREET1\", \n",
    "    #            \"CONTACT_POSTALCODE\", \"CONTACT_ORGANIZATION\", \"CONTACT_ORGTYPE_NUM\"]\n",
    "    \n",
    "    # df_temp1 = sf.sql_getdf(sql_statement, con_tdb, {'df_contacts': df_contacts[aux_list].drop_duplicates(\"CONTACT_ID\")}, column_lower = False)                       \n",
    "    \n",
    "    \n",
    "    #Datenbereinigung 1: Wenn >1 Kontaktkriterien gleiche PART_NR ergeben dann nur 1 Eintrag benötigt    \n",
    "    df_temp1 = df_temp1.drop_duplicates([\"CONTACT_ID\", \"PART_NR\"])\n",
    "        \n",
    "    #Datenbereinigung 2: Kunden mit aktiver Vertragsbeziehung priorisiert, langjährigere Kunden priorisiert\n",
    "    df_temp1[\"CONTACT_CUSTOMER\"] = df_temp1.CONTACT_CUSTOMER.fillna(0)\n",
    "    df_temp1 = df_temp1.sort_values([\"CONTACT_ID\", \"CONTACT_CUSTOMER\", \"PART_NR\"], ascending = [True, False, True]).drop_duplicates(\"CONTACT_ID\")\n",
    "    #df_temp1.head()\n",
    "    \n",
    "    #1.6 Partner-Matching (fuzzy) - hier folgt normalerweise fuzzy matching - wurde hier erstmal weggelassen\n",
    "    \n",
    "    #Datenbereinigung 2: Kunden mit aktiver Vertragsbeziehung priorisiert, Kunden mit besserem Match_Score priorisiert\n",
    "    #d f_temp2[\"CONTACT_CUSTOMER\"] = df_temp2.CONTACT_CUSTOMER.fillna(0)\n",
    "    # df_temp2 = df_temp2.sort_values([\"CONTACT_ID\", \"CONTACT_CUSTOMER\", \"MATCH_SCORE\"], ascending = [True, False, False]).drop_duplicates(\"CONTACT_ID\")\n",
    "    \n",
    "    # einfacher Trick - damit der Code läuft (ohne Fuzzy Matching) - einfach df_temp2 von df_temp1 kopieren\n",
    "    #df_temp2 = df_temp1\n",
    "    #df_temp2.head()\n",
    "    \n",
    "    #1.7 Zusammenführung mit df_contacts\n",
    "    print(\"1.7\")\n",
    "    # 20190508*gep org df_contacts = df_contacts.merge(df_temp1.append(df_temp2), how = \"left\", on = \"CONTACT_ID\")\n",
    "    \n",
    "    # für Testzwecke nach Hybris nur identifizierte Customer schicken, keine Prospects, da diese dann im Hybris/CRM neu \n",
    "    # angelegt werden\n",
    "    df_contacts = df_contacts.merge(df_temp1, how = \"left\", on = \"CONTACT_ID\")\n",
    "    # Attention: for test purposes only real customers and no prospects \n",
    "    # otherwise new partner numbers will be created but makes no sense\n",
    "    df_contacts = df_contacts[df_contacts['PART_NR'].notnull()]\n",
    "    df_contacts.loc[df_contacts.astype(str).drop_duplicates().index]\n",
    "    \n",
    "    df_contacts[\"MATCH_SCORE\"] = df_contacts.MATCH_SCORE.fillna(0)\n",
    "    df_contacts[\"CONTACT_CUSTOMER\"] = df_contacts.CONTACT_CUSTOMER.fillna(0)\n",
    "    df_contacts[\"CONTACT_PARTNER\"] = df_contacts.PART_NR.notnull().astype(int)\n",
    "    \n",
    "    #2   Zusammenführung Projekte & Kontakte\n",
    "    #2.1 Bestimmung führender Partner für Lead (Bauherr)\n",
    "    print(\"2.1\")\n",
    "    df_temp = df_contacts[df_contacts.CONTACT_TYPE.isin([\"Bauherr\", \"Bauherrenvertreter\", \"Architekt / Planer\", \"Generalunternehmung\"])].copy()\n",
    "    df_temp[\"AUX_CONTACT_TYPE\"] = df_temp.CONTACT_TYPE.map({\"Bauherr\": 1, \"Bauherrenvertreter\": 2, \"Architekt / Planer\": 4, \"Generalunternehmung\": 3})\n",
    "    df_temp = df_temp.sort_values([\"PROJECT_ID\", \"AUX_CONTACT_TYPE\", \"CONTACT_CUSTOMER\", \"MATCH_SCORE\", \"CONTACT_GENDER\"], \n",
    "                                  ascending = [True, True, False, False, False]).drop_duplicates(\"PROJECT_ID\")   \n",
    "    \n",
    "    aux_index = df_temp.index #Hilfsindex für Ausschluss dieser Rollen bei den aux_roles\n",
    "    \n",
    "    #2.2 Anschlüsselung führender Partner an Projekt (und Entfernen von Projekte ohne führenden Partner)\n",
    "    print(\"2.2\")\n",
    "    df_projects = df_projects.merge(df_temp, on = \"PROJECT_ID\", how = \"inner\") \n",
    "     \n",
    "#except:\n",
    "#    print(\"error\")    \n",
    "\n",
    "    if df_projects.PROJECT_ID.count() == 0:\n",
    "        print(\"no valid building projects data found\")\n",
    "        raise Exception\n",
    "    \n",
    "    #Projekte mit Bauherren im Ausland werden ausgeschlossen\n",
    "    df_projects[\"AUX_CH\"] = df_projects.apply(lambda row: 1 if ((row.CONTACT_TYPE == \"Bauherr\") & (row.CONTACT_COUNTRY != \"CH\")) else 0, axis = 1)\n",
    "    df_projects = df_projects[df_projects.AUX_CH == 0]\n",
    " \n",
    "    #2.3 Fokus auf 1 Lead pro CONTACT_ID & Tag (Architektur-Defizit Pilot-Schnittstelle)\n",
    "    print(\"2.3\")\n",
    "    df_projects = df_projects.drop_duplicates(\"CONTACT_ID\") \n",
    "    \n",
    "    #2.4 Anschlüsselung zusätzlicher Rollen an Projekt\n",
    "    print(\"2.4\")\n",
    "    df_auxroles = df_contacts.copy()\n",
    "    \n",
    "    auxroles_list = [[[\"Bauherr\"],                                \"ADD_BAUHERR_\"],\n",
    "                     [[\"Bauherrenvertreter\"],                     \"ADD_BAUHERRENVERTRETER_\"],\n",
    "                     [[\"Architekt / Planer\"],                     \"ADD_ARCHITEKT_\"],\n",
    "                     [[\"Bauingenieur\"],                           \"ADD_BAUINGENIEUR_\"], \n",
    "                     [[\"Generalunternehmung\", \"Bauunternehmung\"], \"ADD_GENERALUNTERNEHMUNG_\"]]\n",
    "    \n",
    "    aux_list = [\"CONTACT_ID\", \"CONTACT_GENDER\", \"CONTACT_FIRSTNAME\", \"CONTACT_LASTNAME\",\n",
    "                \"CONTACT_ORGANIZATION\", \"CONTACT_STREET1\", \"CONTACT_POSTALCODE\", \"CONTACT_CITY\",\n",
    "                \"CONTACT_PHONE1\", \"CONTACT_PHONE2\", \"CONTACT_EMAIL1\",  \"CONTACT_ORGTYPE\", \n",
    "                \"CONTACT_CUSTOMER\", \"CONTACT_PARTNER\", \"PROJECT_ID\", \"PART_NR\"]\n",
    "    \n",
    "    for i in auxroles_list:\n",
    "        df_temp = df_auxroles[df_auxroles.CONTACT_TYPE.isin(i[0])]\n",
    "        df_temp = df_temp.sort_values([\"PROJECT_ID\", \"CONTACT_CUSTOMER\", \"MATCH_SCORE\", \"CONTACT_GENDER\"], \n",
    "                                        ascending = [True,  False, False, False]).drop_duplicates(\"PROJECT_ID\")[aux_list]\n",
    "      \n",
    "        #Umbenennen Felder in aux_list\n",
    "        df_temp = df_temp.rename(columns = dict(zip(aux_list, [j.replace(\"CONTACT_\", i[1]).replace(\"PART_NR\", i[1] + \"PART_NR\") for j in aux_list])) )\n",
    "        \n",
    "        #Anschlüsselung an df_projects\n",
    "        df_projects = df_projects.merge(df_temp, how = \"left\", on = \"PROJECT_ID\")\n",
    "        \n",
    "        #Befüllung Leerwerte (für spätere Freitext-Generierung)\n",
    "        exclude_list = [\"PART_NR\", \"CONTACT_ID\", \"CONTACT_CUSTOMER\", \"CONTACT_PARTNER\", \"PROJECT_ID\"]\n",
    "        for j in [k for k in aux_list if k not in exclude_list]:\n",
    "            aux_val = j.replace(\"CONTACT_\", \"\")\n",
    "            df_projects[i[1] + aux_val] = df_projects[i[1] + aux_val].fillna(\"\").astype(str)           \n",
    "        \n",
    "        df_projects[i[1] + \"PART_NR\"] = df_projects[i[1] + \"PART_NR\"].apply(lambda x: \"\" if pd.isnull(x) else str(int(x)))\n",
    "    \n",
    "        #Flagging Verfügbarkeit einzelner Rollen\n",
    "        df_projects[i[1] + \"AVAILABLE\"] = 0\n",
    "        aux_index = df_contacts[df_contacts.CONTACT_TYPE.isin(i[0])].PROJECT_ID.unique()\n",
    "        aux_index = df_projects[df_projects.PROJECT_ID.isin(aux_index)].index\n",
    "        df_projects.loc[aux_index, i[1] + \"AVAILABLE\"] = 1\n",
    "    \n",
    "    #3   Feature Engineering\n",
    "    #3.1 Anschlüsselung GA- & Kanal-Information\n",
    "    print(\"3.1\")\n",
    "    df_projects[\"GA\"] = df_projects.ADDRESS_POSTALCODE.map(dict(zip(plz_df.PLZ, plz_df.GA)))\n",
    "    df_projects[\"GA_LANGUAGE\"] = df_projects[\"GA\"].apply(lambda x: ga_dict.get(x, \"DE\"))\n",
    "    df_projects[\"KANAL\"] = \"AD\"\n",
    "    df_projects[\"CAMPAIGN_NAME\"] = \"80017_Bindexis\"\n",
    "    \n",
    "    #3.2 Trennung Festnetz- & Mobilnummer (für Schnittstelle)\n",
    "    print(\"3.2\")\n",
    "    def tel_identifier(row, tel_type):\n",
    "        tel_list = [row[\"CONTACT_PHONE{}\".format(j)] for j in range(1,4)] \n",
    "        mobile_list = [\"+4175\", \"+4176\", \"+4177\", \"+4178\", \"+4179\"]\n",
    "        \n",
    "        if tel_type == \"mobile\": aux_list = [i for i in tel_list if i[:5]     in mobile_list]\n",
    "        else:                    aux_list = [i for i in tel_list if i[:5] not in mobile_list]   \n",
    "        \n",
    "        if len(aux_list) > 0: return aux_list[0]\n",
    "        else:                 return None \n",
    "            \n",
    "    df_projects[\"AUX_PHONE_FIX\"]    = df_projects.apply(lambda row: tel_identifier(row, \"fix\"), axis = 1)\n",
    "    df_projects[\"AUX_PHONE_MOBILE\"] = df_projects.apply(lambda row: tel_identifier(row, \"mobile\"), axis = 1)\n",
    "    \n",
    "    print(\"3.3\")\n",
    "    df_projects[\"AUX_STREET\"]      = df_projects.CONTACT_STREET1.apply(lambda x: aux.split_housenumber_street(x)[0])\n",
    "    df_projects[\"AUX_HOUSENUMBER\"] = df_projects.CONTACT_STREET1.apply(lambda x: aux.split_housenumber_street(x)[1])\n",
    "    \n",
    "    #3.4 Generierung Freitext-Feld\n",
    "    print(\"3.4\")\n",
    "    def freitext_generator(row):\n",
    "        sprach_dict = {\"PROJEKT\":             {\"DE\": \"PROJEKT\", \"FR\": \"PROJET\", \"IT\": \"PROGETTO\"},\n",
    "                       \"BESCHREIBUNG\":        {\"DE\": \"BESCHREIBUNG\", \"FR\": \"DESCRIPTION\", \"IT\": \"DESCRIZIONE\"},\n",
    "                       \"PARZELLE\":            {\"DE\": \"PARZELLE\", \"FR\": \"PARCELLE\", \"IT\": \"PARCELLA\"},\n",
    "                       \"ADRESSE\":             {\"DE\": \"ADRESSE\", \"FR\": \"ADRESSE\", \"IT\": \"INDIRIZZO\"},\n",
    "                       \"BAUWERT\":             {\"DE\": \"BAUWERT\", \"FR\": \"VALEUR DE CONSTRUCTIION\", \"IT\": \"VALORE DI COSTRUZIONE\"},\n",
    "                       \"ANZ. APARTMENTS\":     {\"DE\": \"ANZ. APARTMENTS\", \"FR\": \"NOMBRE D’APPARTEMENTS\", \"IT\": \"NUMERO DI APPARTAMENTI\"},\n",
    "                       \"ZUSÄTZL.\":            {\"DE\": \"ZUSÄTZL.\", \"FR\": \"SUPPLÉMENTAIRE\", \"IT\": \"ULTERIORE\"},\n",
    "                       \"BAUHERR\":             {\"DE\": \"BAUHERR\", \"FR\": \"MAÎTRE D'OUVRAGE\", \"IT\": \"COMMITTENTE DELL'OPERA\"},\n",
    "                       \"BAUHERRENVERTRETER\":  {\"DE\": \"BAUHERRENVERTRETER\", \"FR\": \"REPRÉSENTANT DU MAÎTRE D'OUVRAGE\", \"IT\": \"RAPPRESENTANTE DEL COMMITTENTE\"},\n",
    "                       \"ARCHITEKT\":           {\"DE\": \"ARCHITEKT\", \"FR\": \"ARCHITECTE\", \"IT\": \"ARCHITETTO\"},\n",
    "                       \"BAUINGENIEUR\":        {\"DE\": \"BAUINGENIEUR\", \"FR\": \"INGÉNIEUR CIVIL\", \"IT\": \"INGEGNERE CIVILE\"},\n",
    "                       \"GENERALUNTERNEHMUNG\": {\"DE\": \"GENERALUNTERNEHMUNG\", \"FR\": \"ENTREPRISE GÉNÉRALE\", \"IT\": \"IMPRENDITORE GENERALE\"},\n",
    "                       \"PHONE1\":              {\"DE\": \"TELEFON\", \"FR\": \"TELEFON\", \"IT\": \"TELEFON\"},\n",
    "                       \"EMAIL1\":              {\"DE\": \"EMAIL\", \"FR\": \"EMAIL\", \"IT\": \"EMAIL\"},\n",
    "                       \"PART_NR\":             {\"DE\": \"PART_NR\", \"FR\": \"PART_NR\", \"IT\": \"PART_NR\"}}\n",
    "    \n",
    "        #Basisinformationen Objekt\n",
    "        x = \"\"\"{0}: {1} \n",
    "        {3} \n",
    "        {5}\"\"\".format(sprach_dict[\"PROJEKT\"][row.GA_LANGUAGE], \n",
    "                   row.PROJECT_TITLE,\n",
    "                   sprach_dict[\"BESCHREIBUNG\"][row.GA_LANGUAGE],\n",
    "                   row.PROJECT_DESCRIPTION,\n",
    "                   sprach_dict[\"PARZELLE\"][row.GA_LANGUAGE],\n",
    "                   row.PROJECT_PARCELID)\n",
    "        \n",
    "        #Adresse\n",
    "        aux_address = (row.ADDRESS_STREET1 + \" \" + row.ADDRESS_STREET2 + \" \" + row.ADDRESS_STREET3).rstrip()\n",
    "        aux_address += \", \" + str(row.ADDRESS_POSTALCODE) + \" \" + row.ADDRESS_CITY\n",
    "        x += \"{0}: {1}\\\\r\\\\n\".format(sprach_dict[\"ADRESSE\"][row.GA_LANGUAGE], aux_address) \n",
    "        \n",
    "        if row.PROJECT_VALUE > 0:\n",
    "            aux_val = locale.format(\"%d\", row.PROJECT_VALUE, grouping = True)\n",
    "            x += \"{0}: {1} CHF\\\\r\\\\n\".format(sprach_dict[\"BAUWERT\"][row.GA_LANGUAGE], aux_val)\n",
    "            \n",
    "        if row.PROJECT_APARTMENTS > 0: \n",
    "            aux_val = str(row.PROJECT_APARTMENTS)\n",
    "            x += \"{0}: {1}\\\\r\\\\n\".format(sprach_dict[\"ANZ. APARTMENTS\"][row.GA_LANGUAGE], aux_val)           \n",
    "        \n",
    "        #Zusätzliche Rollen\n",
    "        for i in [\"ADD_BAUHERR_\", \"ADD_BAUHERRENVERTRETER_\", \"ADD_ARCHITEKT_\", \"ADD_BAUINGENIEUR_\", \"ADD_GENERALUNTERNEHMUNG_\"]:                        \n",
    "            if pd.notnull(row[i + \"ID\"]): \n",
    "                #Konkatinieren Name der Organisation / Person\n",
    "                aux_name = \"{0} {1} {2}\".format(row[i + \"ORGANIZATION\"], row[i + \"FIRSTNAME\"], row[i + \"LASTNAME\"])\n",
    "                aux_name = aux_name.strip()\n",
    "                \n",
    "                #Konkatinieren PART_NR und Kontaktinformationen\n",
    "                aux_contact = []\n",
    "                for z in [\"PART_NR\", \"PHONE1\", \"EMAIL1\"]: \n",
    "                    if row[i + z] != \"\": aux_contact += [\"{0}: {1}\".format(sprach_dict[z][row.GA_LANGUAGE], row[i + z])]              \n",
    "                \n",
    "                if aux_contact == []: aux_contact = \"\"\n",
    "                else: aux_contact = \"({0})\".format(\", \".join(aux_contact))\n",
    "    \n",
    "                #Konkatinieren Adressinformation\n",
    "                aux_address = \"{0}, {1} {2}\".format(row[i + \"STREET1\"], row[i + \"POSTALCODE\"], row[i + \"CITY\"])\n",
    "                aux_address = aux_address.strip()\n",
    "                \n",
    "                x += \"{0} {1}: {2} {3}, {4}\\\\r\\\\n\".format(sprach_dict[\"ZUSÄTZL.\"][row.GA_LANGUAGE], \n",
    "                                                          sprach_dict[i[4:-1]][row.GA_LANGUAGE],\n",
    "                                                          aux_name, aux_contact, aux_address)\n",
    "                                        \n",
    "        return x.replace(\";\", \",\").replace(\"\\\\r\\\\n\", \", \")\n",
    "        \n",
    "    df_projects[\"BINDEXIS_AD_LEAD_INFO\"] = df_projects.apply(lambda row: freitext_generator(row), axis = 1)\n",
    "    \n",
    "    #3.5 Generierung Felder für Hybris-Schnittstelle\n",
    "    print(\"3.5\")\n",
    "    #Interaction\n",
    "    df_projects[\"IAKTN_ART_CDYMKT\"] = \"2\"                       \n",
    "    df_projects[\"KOMKN_MEDIUM_CDYMKT\"] = \"3\"                    \n",
    "    df_projects[\"REF_BO_KLSFKN_CDU\"] = \"BO-BAUGES-ID-HYBRIS\"    #TBD\n",
    "    \n",
    "    #Contact\n",
    "    df_projects[\"IAKTN_KNTKT_REF_TYP_CDYMKT\"] = \"2\"              \n",
    "    df_projects[\"FCT_IAKTN_KNTKT_REF_ID\"] = df_projects.PART_NR.apply(lambda x: str(int(x)) if pd.notnull(x) else \"\") \n",
    "    df_projects[\"FCT_IAKTN_KNTKT_REF_TYP_CDYMKT\"] = df_projects.FCT_IAKTN_KNTKT_REF_ID.apply(lambda x: \"\" if x == \"\" else \"4\")        \n",
    "    df_projects[\"GPART_TYP_CDGPD\"] = df_projects.CONTACT_ORGTYPE.map({\"P\": \"1\", \"U\": \"2\"})\n",
    "    df_projects[\"ANRED_ASHRFT_CDGPD\"] = df_projects.CONTACT_GENDER.map({\"Herr\": \"1\", \"Frau\": \"2\"})\n",
    "    df_projects[\"SEX_CDU\"] = df_projects.CONTACT_GENDER.map({\"Herr\": \"1\", \"Frau\": \"2\"})\n",
    "    df_projects[\"NAME_NNP_ZEILE1\"] = df_projects.CONTACT_ORGANIZATION.str[:35]\n",
    "    \n",
    "    #4   Qualitätsfilterung Baugesuche & Deselektion   \n",
    "    #4.1 Filterung auf bestehende Kunden\n",
    "    df_projects = df_projects[df_projects.CONTACT_CUSTOMER == 1].copy()\n",
    "    \n",
    "    #4.2 Qualitäts-Ausschlüsse\n",
    "    print(\"4.2\")\n",
    "    #A) Kategorisierung Projektkosten\n",
    "    df_projects['EXCLUDE_LOWVALUE'] = df_projects.PROJECT_VALUE.apply(lambda x: 1 if ((x > 0) & (x <= 200000)) else 0)\n",
    "    \n",
    "    #B) Identifikation von Abbruchprojekten\n",
    "    change_list = ['BUILDING_DEVELOPMENT_NEUBAU', 'BUILDING_DEVELOPMENT_UMBAU','BUILDING_DEVELOPMENT_ANBAU',\n",
    "                   'BUILDING_DEVELOPMENT_ÄUSSERE_VERÄNDERUNGEN', 'BUILDING_DEVELOPMENT_ABBRUCH']        \n",
    "    \n",
    "    def building_develop_sum_1(summe, change):\n",
    "        if ((summe == 1) & (change == 1)): return 1\n",
    "        else:                              return 0                             \n",
    "    \n",
    "    df_projects['BUILDING_DEVELOPMENT_SUM'] = df_projects[change_list].sum(axis = 1)                          \n",
    "    df_projects[\"EXCLUDE_ABBRUCH\"] = df_projects.apply(lambda row: building_develop_sum_1(row[\"BUILDING_DEVELOPMENT_SUM\"], row[\"BUILDING_DEVELOPMENT_ABBRUCH\"]), axis = 1)    \n",
    "             \n",
    "    #C) Identifikation von relevanten Objekten\n",
    "    def flag_irrelevant(x, rel, irrel):\n",
    "        if any(i in x for i in irrel):\n",
    "            if any(j in x for j in rel): return 0\n",
    "            else:                        return 1\n",
    "        else:                            return 0  \n",
    "        \n",
    "    df_projects[\"BUILDING_DETAIL\"] = df_projects.BUILDING_DETAIL.fillna('unknown')                \n",
    "            \n",
    "    #Definition der Positiv- und Negativ-Liste der Objekte\n",
    "    relevant_objects =      ['Arztpraxen und Ärztehäuser', \n",
    "                             'Atelier und Studio', \n",
    "                             'Ausstellungsbauten', \n",
    "                             'Autowerkstätten', \n",
    "                             'Bauernhäuser', \n",
    "                             'Betriebs- und Gewerbebauten', \n",
    "                             'Bürobauten', \n",
    "                             'Doppel-Einfamilienhäuser', \n",
    "                             'Einfamilienhaus-Siedlungen',\n",
    "                             'Einfamilienhäuser', \n",
    "                             'Elektrische Verteilanlagen',\n",
    "                             'Fitnesscenter',\n",
    "                             'Heizzentralen, Fernwärmeanlagen und Kraftwerkbauten',\n",
    "                             'Herbergen, Jugendherbergen und Massenunterkünfte',\n",
    "                             'Hotel- und Motelbauten',\n",
    "                             'Industriehallen',\n",
    "                             'Industrielle Produktionsbauten',\n",
    "                             'Kantinen',\n",
    "                             'Kino-, Diskothek- und Saalbauten',\n",
    "                             'Ladenbauten',\n",
    "                             'Lager- und Umschlagplätze',\n",
    "                             'Lagerhallen',\n",
    "                             'Lebensmittelproduktion',\n",
    "                             'Mehrfamilienhäuser',\n",
    "                             'Mehrgeschossige Lagerbauten',\n",
    "                             'Privatschwimmbäder, Jacuzzi, Wellness',\n",
    "                             'Raststätten, Cafeterias, Tea-Rooms und Bars',\n",
    "                             'Reihenhäuser',\n",
    "                             'Reithallen',\n",
    "                             'Restaurationsbetriebe',\n",
    "                             'Schlachthöfe',\n",
    "                             'Stallungen und landwirtschaftliche Produktionsanlagen',\n",
    "                             'Supermärkte',\n",
    "                             'Terrassenhäuser',\n",
    "                             'Tiefgaragen und Unterniveaugaragen',\n",
    "                             'Warenhäuser, Einkaufszentren und Showrooms',\n",
    "                             'Wärme- und Kälteverteilanlagen',\n",
    "                             'Wohlfahrtshäuser, Klubhäuser und Kulturzentren',\n",
    "                             'Wohnungen']\n",
    "                            \n",
    "    irrelevant_objects    = ['Abdankungshallen',\n",
    "                             'Alterswohnheime', \n",
    "                             'Alterswohnungen, Alterssiedlungen',\n",
    "                             'Aussenanlagen, Kinderspielplätze und Parkanlagen',\n",
    "                             'Bahnhöfe und Bahnbetriebsbauten, Seilbahnstationen', \n",
    "                             'Banken, Postgebäude und Fernmeldegebäude', \n",
    "                             'Behelfswohnungen',\n",
    "                             'Berghäuser',\n",
    "                             'Berufs- und höhere Fachschulen', \n",
    "                             'Bibliotheken und Staatsarchive', \n",
    "                             'Bootshäuser',\n",
    "                             'Burgen & Schlösser', \n",
    "                             'Busbahnhöfe, Zollanlagen und Wartehallen mit Diensträumen', \n",
    "                             'Campinganlagen',\n",
    "                             'Casino', \n",
    "                             'Deponien',\n",
    "                             'Feuerwehrgebäude',\n",
    "                             'Flughafenbauten',\n",
    "                             'Forschungsinstitute',\n",
    "                             'Freizeitzentren und Jugendhäuser',\n",
    "                             'Friedhofanlagen',\n",
    "                             'Futterlagerräume, Treibhäuser und Silobauten',\n",
    "                             'Garagen und Unterstände',\n",
    "                             'Gartenhäuser', \n",
    "                             'Gemeindehäuser, Rathäuser und Regierungsgebäude',\n",
    "                             'Gerichtsgebäude', \n",
    "                             'Gewächshäuser', \n",
    "                             'Hallen- und Freibäder',\n",
    "                             'Heilbäder und Spezialinstitute',\n",
    "                             'Heilpädagogische Schulen/Sonderschulen',\n",
    "                             'Hochschulen und Universitäten',\n",
    "                             'Kasernen', \n",
    "                             'Kehrichtverbrennungs- und Wiederaufbereitungsanlagen',\n",
    "                             'Keller', \n",
    "                             'Kinder- und Jugendheime',\n",
    "                             'Kinderhorte und Kindergärten',\n",
    "                             'Kirchen und Kapellen',\n",
    "                             'Kirchgemeindehäuser',\n",
    "                             'Klöster',\n",
    "                             'Klubhütten', \n",
    "                             'Kongresshäuser und Festhallen',\n",
    "                             'Konzertbauten und Theaterbauten', \n",
    "                             'Krankenhäuser',\n",
    "                             'Krematorien', \n",
    "                             'Lofts', \n",
    "                             'Mechanisierte Lager und Kühllager', \n",
    "                             'Militäranlagen und militärische Schutzanlagen', \n",
    "                             'Mittelschulen und Gymnasien',\n",
    "                             'Museen und Kunstgalerien',\n",
    "                             'Musikpavillons', \n",
    "                             'Öffentliche WC-Anlagen', \n",
    "                             'Öffentliche Zivilschutzanlagen', \n",
    "                             'Parkhäuser und Einstellhallen',\n",
    "                             'Parkplätze und Abstellplätze', \n",
    "                             'Pavillons',\n",
    "                             'Pflegeheime, Sanatorien und Rehabilitationszentren',\n",
    "                             'Polizeieinsatzgebäude und Untersuchungsgefängnisse',\n",
    "                             'Post- und Logistikterminale',\n",
    "                             'Primar- und Sekundarschulen',\n",
    "                             'Radio-, Fernseh- und Filmstudios', \n",
    "                             'Sammelstellen',\n",
    "                             'Schuppen und Hütten',\n",
    "                             'Silobauten und Behälter', \n",
    "                             'Sportanlagen, Turn- und Mehrzweckanlagen',\n",
    "                             'Strafvollzugsanstalten', \n",
    "                             'Strassenverkehrsgebäude',\n",
    "                             'Studenten- und Lehrlingswohnheime',\n",
    "                             'Tagesheime und geschützte Werkstätten',\n",
    "                             'Tankanlagen und Tankstellen', \n",
    "                             'Tierheime und Veterinärstationen',\n",
    "                             'Tierspitäler',\n",
    "                             'Tribünenbauten und Garderobengebäude', \n",
    "                             'Universitätskliniken',\n",
    "                             'unknown',\n",
    "                             'Verteilanlagen für Trinkwasser', \n",
    "                             'Verteilzentralen', \n",
    "                             'Verwaltungsgebäude und Rechenzentren',\n",
    "                             'Wasseraufbereitungsanlagen', \n",
    "                             'Wellness', \n",
    "                             'Werkhöfe', \n",
    "                             'Wintergärten und Balkonverglasungen', \n",
    "                             'Zeughäuser',\n",
    "                             'Zoologische und botanische Gärten, Tierhäuser']  \n",
    "                                         \n",
    "    df_projects['EXCLUDE_IRRELEVANT_OBJECT'] = df_projects.BUILDING_DETAIL.apply(lambda x: flag_irrelevant(x, relevant_objects, irrelevant_objects))                       \n",
    "    \n",
    "    #D) Identifikation von relevanten Subjekten     \n",
    "    irrelevant_subjects = ['wärmepumpe', 'pompa di calore', 'termopompe', 'pompe à chaleur', \n",
    "                           'fenster', 'finestra', 'fenêtre', \n",
    "                           'dach', 'tetto', 'toit', \n",
    "                           'fassade', 'facciata', 'façade', \n",
    "                           'heizung', 'riscaldamento', 'chauffage', \n",
    "                           'kamin', 'caminetto', 'caminetti', 'cheminée', \n",
    "                           'wasser', 'acqua', ' eau', \n",
    "                           'sitzplatz', 'terrasse', 'terrazzo', 'sedile', 'banc', \n",
    "                           'balkon', 'balcone', 'balcon', \n",
    "                           'unterstand', 'rifugio', 'subalterno', 'abri', 'container', \n",
    "                           'wartung', 'manutenzione', 'mantenimento', 'entretien', \n",
    "                           'transformator', 'trasformatore', 'alimentatore', 'transformatrice', 'transformateur',\n",
    "                           'leuchtkasten', 'postomat', 'box luce', 'caisson lumineux',\n",
    "                           'pergola',\n",
    "                           'sonnensegel', 'tenda', 'tende', 'banne',\n",
    "                           'schaukel', 'altalene', 'balançoire',\n",
    "                           'gartenhaus', 'casa estiva', \"maison d'été\",\n",
    "                           'sichtschutz', 'vita privata', 'intimité',\n",
    "                           'verglasung', 'vetri', 'vitrage',\n",
    "                           'treppe', 'scala', 'escalier',\n",
    "                           'windfang', \n",
    "                           'erdsonde', \n",
    "                           'spielplatz', 'terreno di gioco', 'cour de récréation',\n",
    "                           'gewächshaus', 'serra', 'serre',\n",
    "                           'zaun', 'recinto', 'clôture',\n",
    "                           'whirlpool', \n",
    "                           'klimaanlage', 'aria condizionata', 'climatisation',\n",
    "                           'bienenhaus', 'apiario', 'rucher']\n",
    "                           \n",
    "    relevant_subjects = ['einfamilienhaus', 'single famiglia', 'casa monofamiliare', 'maison individuelle', \n",
    "                         'zweifamilienhaus', 'casa bifamiliare', 'deux maison familiale', 'maison double',\n",
    "                         'dreifamilienhaus', 'casa trifamiliare', 'casa per tre famiglie', 'maison de trois familles', 'maison à trois logements',\n",
    "                         'mehrfamilienhaus', 'palazzina', 'casa per immobili', 'maison appartement', \"Immeuble d'habitation\",\n",
    "                         'wohnhaus', 'edificio residenziale', 'abitazione', \"immeuble d'habitation\", 'habitation', \n",
    "                         'gebäude', 'costruzione', 'edificio', 'bâtiment', 'immeuble', \n",
    "                         'wohnung', 'appartamento', 'alloggiamento', 'appartement', \n",
    "                         'spital', 'krankenhaus', 'klinik', 'ospedale', 'clinica', 'hôpital', 'clinique', \n",
    "                         'mehrzweck', 'multiuso', 'multifunzionale', 'polyvalent', \n",
    "                         'kongress', 'congresso', 'congrès', \n",
    "                         'lagerhalle', 'magazzino', 'deposito', 'entrepôt', \n",
    "                         'dachgeschoss', 'soffitta', 'piano mansarda', 'grenier', \n",
    "                         'dachaufstockung', 'dachaufbau', 'estensione tetto', 'alzare il tetto', 'surélever le toit', 'extension',\n",
    "                         'photovoltaik', 'fotovoltaico', 'photovoltaïque', 'photovoltaique', \n",
    "                         'wasserreservoir', 'serbatoio di acqua', 'bacino idrico', \"réservoir d'eau\", \n",
    "                         'wasserversorgung', \"approvvigionamento idrico\", 'approvisionnement en eau', 'raccordement eau',\n",
    "                         'pumpwerk', 'stazione di pompaggio', 'station de pompage',\n",
    "                         'château', 'chateau', 'castello', 'chapiteau', 'niveau', 'wasserkraftwerk', \n",
    "                         'construction logement', 'constructione urbane']                       \n",
    "       \n",
    "    df_projects['EXCLUDE_IRRELEVANT_SUBJECT'] = df_projects.PROJECT_TITLE.apply(lambda x: flag_irrelevant(x.lower(), relevant_subjects, irrelevant_subjects))\n",
    "    \n",
    "    #E) Zusammenführung Ausschluskriterien\n",
    "    def aggregate_objectsubject(obj, sub):\n",
    "        if (obj == 0):\n",
    "            if (sub == 1): return 1\n",
    "            else:          return 0\n",
    "        else:              return 1\n",
    "    \n",
    "    df_projects[\"EXCLUDE_TOTAL\"] = df_projects.apply(lambda row: aggregate_objectsubject(row.EXCLUDE_IRRELEVANT_OBJECT, row.EXCLUDE_IRRELEVANT_SUBJECT), axis = 1)\n",
    "    \n",
    "    aux_index = df_projects[(df_projects.EXCLUDE_LOWVALUE == 1) | (df_projects.EXCLUDE_ABBRUCH == 1)].index\n",
    "    df_projects.loc[aux_index, \"EXCLUDE_TOTAL\"] = 1\n",
    "    \n",
    "    df_projects[\"KEEP_TOTAL\"] = df_projects.EXCLUDE_TOTAL.apply(lambda x: 1 - x)\n",
    "    \n",
    "    #4.2 Aufteilung Kunden / Prospects & Anspielung df_cust an Tranche\n",
    "    print(\"4.2\")    \n",
    "  \n",
    "    #type(campaign)\n",
    "    # tranche.tranche_df.head()\n",
    "    #for att in dir(tranche):\n",
    "    #    print (att, getattr(tranche,att)) \n",
    "    \n",
    "    #5   Kanalausspielung\n",
    "    #5.1 Kanalzuordnung & Bildung Kontrollgruppe (für Propsects noch separat)\n",
    "    print(\"5.1\")\n",
    "    df_projects[\"TRANCHE_KG\"] = df_projects.PART_NR.apply(lambda x: 1 if random.random() < prospect_sharekg else 0)\n",
    "    df_projects[\"TARGETGROUP\"] = df_projects.TRANCHE_KG.map({1:\"0\", 0:\"1\"})\n",
    "    # df_projects[\"TRANCHE_DATUM\"] = tranche.tranche_timestamp\n",
    "    # df_projects[\"TRANCHE_NUMMER\"] = tranche.tranche_number\n",
    "    df_projects[\"KANAL\"] = df_projects.KEEP_TOTAL.apply(lambda x: \"AD\" if x == 1 else \"AUSSCHLUSS\")       \n",
    "    df_projects[\"CHANNEL\"] = \"LEAD\"\n",
    "\n",
    "    tranche = cm.Tranche(campaign, df_projects, stage)\n",
    "\n",
    "    tranche.tranche_df[\"TRANCHE_DATUM\"] = tranche.tranche_timestamp\n",
    "    tranche.tranche_df[\"TRANCHE_NUMMER\"] = tranche.tranche_number\n",
    "    \n",
    "    #5.2 Aktualisierung Tranche & KPIs\n",
    "    print(\"5.2\")\n",
    "    tranche.kpi_customers_identified     = df_projects.shape[0]\n",
    "    tranche.kpi_customers_selected       = tranche.tranche_df.shape[0]\n",
    "    tranche.kpi_size_targetgroup         = tranche.tranche_df[tranche.tranche_df.TRANCHE_KG == 0].shape[0]\n",
    "    tranche.kpi_size_controlgroup        = tranche.tranche_df[tranche.tranche_df.TRANCHE_KG == 1].shape[0]\n",
    "    \n",
    "    #5.3 Aktualisierung NVP-Informationen auf Tranchen-Objekt\n",
    "    print(\"5.3\") \n",
    "    tranche.tranche_df[\"CHANNEL\"] = \"LEAD\"\n",
    "    tranche.tranche_df[\"TARGETGROUP\"] = tranche.tranche_df.TRANCHE_KG.map({1:\"0\", 0:\"1\"})\n",
    "    \n",
    "    #5.4 Anspielung Pilotschnittstelle\n",
    "    print(\"5.4\")      \n",
    "    interaction_mapping = {\"iaktn_art_cdymkt\":      \"IAKTN_ART_CDYMKT\", \n",
    "                           \"komkn_medium_cdymkt\":   \"KOMKN_MEDIUM_CDYMKT\", \n",
    "                           \"ref_bo_klsfkn_cdu\":     \"REF_BO_KLSFKN_CDU\",\n",
    "                           \"ref_bo_id\":             \"PROJECT_ID\", \n",
    "                           \"ntz_txt_lang\":          \"BINDEXIS_AD_LEAD_INFO\"}\n",
    "    \n",
    "    # print(type(interaction_mapping))\n",
    "    #if type(interaction_mapping) == dict:\n",
    "    #    aux_dict2 = {}\n",
    "    #    for i in interaction_mapping.keys(): \n",
    "    #        print(i)\n",
    "    #        print(row[interaction_mapping[i]])\n",
    "    #        aux_dict2[i] = row[interaction_mapping[i]]  \n",
    "    #        print(aux_dict[i])\n",
    "    \n",
    "    contact_mapping = {\"iaktn_kntkt_ref_id\":             \"CONTACT_ID\",\n",
    "                       \"iaktn_kntkt_ref_typ_cdymkt\":     \"IAKTN_KNTKT_REF_TYP_CDYMKT\",                                       \n",
    "                       \"fct_iaktn_kntkt_ref_id\":         \"FCT_IAKTN_KNTKT_REF_ID\",\n",
    "                       \"fct_iaktn_kntkt_ref_typ_cdymkt\": \"FCT_IAKTN_KNTKT_REF_TYP_CDYMKT\",\n",
    "                       \"gpart_typ_cdgpd\":                \"GPART_TYP_CDGPD\",  \n",
    "                       \"name_nnp_zeile1\":                \"NAME_NNP_ZEILE1\",       \n",
    "                       \"anred_ashrft_cdgpd\":             \"ANRED_ASHRFT_CDGPD\",\n",
    "                       \"vname\":                          \"CONTACT_FIRSTNAME\",\n",
    "                       \"nname\":                          \"CONTACT_LASTNAME\",         \n",
    "                       \"sex_cdu\":                        \"SEX_CDU\",\n",
    "                       \"spra_cdi_korrz\":                 \"GA_LANGUAGE\",              \n",
    "                       \"email_adr\":                      \"CONTACT_EMAIL1\",\n",
    "                       \"tel_nr_kompl\":                   \"AUX_PHONE_FIX\",\n",
    "                       \"tel_nr_kompl_mobil\":             \"AUX_PHONE_MOBILE\",\n",
    "                       \"str_name\":                       \"AUX_STREET\",\n",
    "                       \"haus_nr_kompl\":                  \"AUX_HOUSENUMBER\",     \n",
    "                       \"plz\":                            \"CONTACT_POSTALCODE\",\n",
    "                       \"ort_name\":                       \"CONTACT_CITY\",\n",
    "                       \"land_cdi\":                       \"CONTACT_COUNTRY\"}\n",
    "    \n",
    "    nvp_mapping = {\"CHANNEL\":      \"CHANNEL\", \n",
    "                   \"CAMPAIGNNAME\": \"CAMPAIGN_NAME\", \n",
    "                   \"TARGETGROUP\":  \"TARGETGROUP\"}\n",
    "    \n",
    "    # FROM DATALAKE 1.0 - Achtung falsch, nicht nehmen, gemäss Sandor und Reto Haag sollten\n",
    "    # die neuen Funktionen nehmen siehe soa_gc2 \n",
    "    # wir benötigen noch ein Zertifikat --> Sandor \n",
    "    # wichtig - sicher sein, dass als Stage ACC übergeben wird, dann sollte die richtige URL \n",
    "    # für Hybris aus Standard in Standardfunktion soa_marketinginteraktiondatenpush_1\n",
    "    # einzig der Parameter für das CERT-File muss neu evtl. mitgegeben werden, oder aber wir\n",
    "    # überschreiben in der Funktion die aktuell Einstellung direkt\n",
    "    \n",
    "    #SOA service call \n",
    "    count_sel = df_projects.PROJECT_ID.count()\n",
    "    print (\"count_sel: \", count_sel)\n",
    "\n",
    "# steps before only for test purposes \n",
    "\n",
    "    if stage == \"ACC\" and run_mode == \"go_hybris\":\n",
    "        if count_sel > 0:\n",
    "            print('nun folgt der SOA Aufruf und tranche.route_to_hybris ...')\n",
    "    # 20190516*gep: näcshte Zeile als Trick damit es keinen Abbruch gibt, da keine echte Kanalaufspielung\n",
    "            tranche.tranche_status = \"3: Konstante\"\n",
    "            tranche.route_to_hybris(interaction_mapping, contact_mapping, nvp_mapping)\n",
    "            print(\"Ausgabe nach run tranche.route_to_hybris\")\n",
    "        else:\n",
    "            print(\"no valid entries to transfer to hybris\")\n",
    "\n",
    "except Exception:\n",
    "    #6   Exception Handling, Backup, Reporting\n",
    "    #6.1 eMail-Benachrichtigung Exception\n",
    "    print(\"6.1\")\n",
    "    print(\"hier normal Aufruf tranche.report_exception(traceback.format_exc()) - im GCP PoC nicht\")\n",
    "    # tranche.report_exception(traceback.format_exc())\n",
    "\n",
    "    print(traceback.format_exc())\n",
    "    \n",
    "else:\n",
    "    #6.2 Erstellung Backup \n",
    "    print(\"6.2\")\n",
    "    #Liste aller Spalten der Tranche zur Generierung der Variablenliste\n",
    "    column_list = list(tranche.tranche_df.columns.values)\n",
    "    \n",
    "    #Variablenliste: Basisinformation\" \n",
    "    #var_list =  [\"KANAL\",  \"TRANCHE_KG\", \"TRANCHE_NUMMER\", \"TRANCHE_DATUM\", \"GA\", \"PART_NR\", \n",
    "    #             \"MATCH_TYPE\", \"MATCH_SCORE\", \"API_RESULT\"]\n",
    "\n",
    "    var_list =  [\"KANAL\",  \"TRANCHE_KG\", \"TRANCHE_NUMMER\", \"TRANCHE_DATUM\", \"GA\", \"PART_NR\", \n",
    "                 \"MATCH_TYPE\", \"MATCH_SCORE\"]\n",
    "    \n",
    "    #Variablenliste: Projekt- & Kontaktinformationen\n",
    "    for k in [\"PROJECT_\", \"DATE_\", \"DETAIL_\", \"ADDRESS_\", \"CONTACT_\"]:\n",
    "        var_list += [i for i in column_list if i.startswith(k)]    \n",
    "    \n",
    "    #Variablenliste: Zusatzinformationen\n",
    "    for k in [\"BUILDING_\", \"ADD_BAUHERR_\", \"ADD_BAUHERRENVERTRETER_\", \"ADD_ARCHITEKT_\", \n",
    "              \"ADD_BAUINGENIEUR_\", \"ADD_GENERALUNTERNEHMUNG_\"]:\n",
    "        var_list += [i for i in column_list if i.startswith(k)]\n",
    "    \n",
    "   \n",
    "# 20190520*gep workaround 62 Speicherung Zeit Parameter bei erfolgreichem Lauf\n",
    "    if stage == \"ACC\" and run_mode == \"go_hybris\": \n",
    "        print(\"6.2: workaround for campaign.backup(tranche, var_list) --> campaign_timelastrun\")\n",
    "    # campaign.backup(tranche, var_list)\n",
    "        try:\n",
    "            campaign_timelastrun = datetime.datetime.now(pytz.timezone('Europe/Zurich'))\n",
    "            filename = \"{}/kampagne.pkl\".format(tempfile.gettempdir())\n",
    "            with open(filename, 'wb') as fp: pickle.dump(campaign_timelastrun, fp)\n",
    "            blob = bucket_cs.blob(path_data_va+'kampagne.pkl')\n",
    "            blob.upload_from_filename(filename)\n",
    "            print(\"6.2 workaround backup try\")\n",
    "        except:\n",
    "            print(\"attention: except true\")\n",
    "        \n",
    "        print(campaign_timelastrun)     \n",
    "    \n",
    "    # workaround zu campaign.backup(tranche, var_list) rück schreiben des Zeitpunkts \n",
    "    # 'gs://axa-ch-raw-dev-dla/bindexis/data/various/kampagne.pkl' \n",
    "    # das rück schreiben erfolgt in der methode \n",
    "\n",
    "    #6.3 eMail-Benachrichtigung erfolgreicher Lauf\n",
    "    #print(\"6.3: tranche.report_success()\")\n",
    "    # tranche.report_success()\n",
    "    \n",
    "    print(\"6.4: trigger run beendet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
