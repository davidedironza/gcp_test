{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'axa-ch-raw-dev-dla'\n",
    "PROJECT = 'axa-ch-datalake-analytics-dev'\n",
    "REGION = 'eu-west6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0   Modul-Import & Parametrierung    \n",
    "#0.1 Modul-Import\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import tempfile\n",
    "import logging\n",
    "from html import unescape\n",
    "from random import randint\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2 Set Stage\n",
    "stage = \"DEV\" #sf.platform_is_server(\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3 Pfadinformationen\n",
    "path_data_va = \"bindexis/data/various/\"\n",
    "path_data_input = \"bindexis/data/input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.4 Set timezone\n",
    "os.environ['TZ'] = 'Europe/Zurich'\n",
    "time.tzset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "2019-05-09 14:26:36.179861+02:00\n"
     ]
    }
   ],
   "source": [
    "# 1   Retrieve Bindexis Data\n",
    "#1.1 Access & Authentication Setup\n",
    "print(\"1.1\")\n",
    "username      = \"TIppisch\"\n",
    "time_now      = datetime.datetime.now(pytz.timezone('Europe/Zurich'))\n",
    "\n",
    "client_cs = storage.Client()\n",
    "bucket_cs = client_cs.get_bucket(BUCKET)\n",
    "try:\n",
    "    filename = \"{}/Parameter_TimeLastRun.pkl\".format(tempfile.gettempdir())\n",
    "    #blob = bucket_cs.blob(path_data_va+'Parameter_TimeLastRun.txt')\n",
    "    #time_last_run = blob.download_as_string()\n",
    "    #time_last_run = datetime.datetime.strptime(time_last_run.decode(\"utf-8\")[:-6], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    blob = bucket_cs.blob(path_data_va+'Parameter_TimeLastRun.pkl')\n",
    "    blob.download_to_filename(filename)\n",
    "    with open(filename, 'rb') as fp: time_last_run = pickle.load(fp)\n",
    "except:\n",
    "    time_last_run      = time_now + datetime.timedelta(days=-2)\n",
    "\n",
    "print(time_last_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"1.2\")\n",
    "#file = \"../certificates/sas_server_keystore.pem\"\n",
    "data = \"\"\"<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:soap=\"http://soap.bindexis.ch/\">\n",
    "           <soapenv:Header>\n",
    "              <soap:AuthHeader>\n",
    "                 <!--Optional:-->\n",
    "                 <soap:UserName>{0}</soap:UserName>\n",
    "                 <!--Optional:-->\n",
    "                 <soap:Password>{1}</soap:Password>\n",
    "              </soap:AuthHeader>\n",
    "           </soapenv:Header>\n",
    "           <soapenv:Body>\n",
    "              <soap:GetProjectSync>\n",
    "                 <!--Optional:-->\n",
    "                 <soap:dateToStart>{2}</soap:dateToStart>\n",
    "              </soap:GetProjectSync>\n",
    "           </soapenv:Body>\n",
    "        </soapenv:Envelope>\"\"\".format(username, \"19e22172\", time_last_run.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "response = requests.post(url     = \"https://soap.bindexis.ch/projectsync.asmx\",\n",
    "#response = requests.post(url     = \"https://esg-opco.medc.services.axa-tech.intraxa:8443/AXACH_BindexisWebService\",\n",
    "                         headers = {'content-type': 'text/xml'},\n",
    "                         #cert    = file,\n",
    "                         data    = data,\n",
    "                         verify  = False)\n",
    "\n",
    "base_text = unescape(response.text)\n",
    "root = ET.fromstring(base_text[base_text.find(\"<?xml\", 1): base_text.find(\"</GetProjectSyncResult>\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.21 Save response.text to GCS\n",
    "blob = bucket_cs.blob(path_data_input+'bindexis_{}.txt'.format(time_now.strftime('%Y%m%d%H%M%S')))\n",
    "blob.upload_from_string(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n"
     ]
    }
   ],
   "source": [
    "#1.3 Extraction Description Dictionary\n",
    "print(\"1.3\")\n",
    "description_dict = {}\n",
    "\n",
    "for i in [\"PlanningStage\", \"SizeUnit\", \"RoofType\", \"ConstructionMaterial\", \"ProjCladdingType\", \n",
    "          \"DevelopmentType\", \"TargetGroupType\", \"Employee\", \"JobFunction\", \"Gender\", \"BuildingType\", \n",
    "          \"HeatingType\", \"ProjectFinalUse\"]:\n",
    "\n",
    "    aux_dict = {}\n",
    "    if i == \"ProjectFinalUse\": values = root.find(\"{0}\".format(i))\n",
    "    else:                      values = root.find(\"{0}Values\".format(i))\n",
    "\n",
    "    if i == \"BuildingType\":\n",
    "        for j in values[1:]: aux_dict[j.find(\"ProId\").text] = [j.find(\"ProDesc\").text, j.find(\"ProParentId\").text]     \n",
    "    else: \n",
    "        for j in values: aux_dict[j.find(\"UtilId\").text] = j.find(\"UtilDesc\").text\n",
    "\n",
    "    description_dict[i] = aux_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4\n"
     ]
    }
   ],
   "source": [
    "#1.4 Definition of Extraction Configuration Files\n",
    "print(\"1.4\")\n",
    "canton_dict = {'CH01': 'AG',     'CH02': 'AI',     'CH03': 'AR',     'CH04': 'BE',     'CH05': 'BL', \n",
    "               'CH06': 'BS',     'CH08': 'FR',     'CH09': 'GE',     'CH10': 'GL',     'CH11': 'GR', \n",
    "               'CH12': 'JU',     'CH13': 'LU',     'CH14': 'NE',     'CH15': 'NW',     'CH16': 'OW',     \n",
    "               'CH17': 'SG',     'CH18': 'SH',     'CH19': 'SO',     'CH20': 'SZ',     'CH21': 'TG',             \n",
    "               'CH22': 'TI',     'CH23': 'UR',     'CH24': 'VD',     'CH25': 'VS',     'CH26': 'ZG',     \n",
    "               'CH27': 'ZH'}  \n",
    "\n",
    "project_list =   [[\"PROJECT_ID\",             \"j.find('ProjId').text\",                          \"STRING\"  ],\n",
    "                  [\"PROJECT_TITLE\",          \"j.find('ProjTitle').text\",                       \"STRING\"  ],   \n",
    "                  [\"PROJECT_PARCELID\",       \"j.find('ProjOfficialId').text\",                  \"STRING\"  ],  \n",
    "                  [\"PROJECT_LANGUAGE\",       \"j.find('ProjLanguageId').text[:2].upper()\",      \"STRING\"   ],\n",
    "                  [\"PROJECT_DESCRIPTION\",    \"j.find('ProjDesc').text\",                        \"STRING\"],\n",
    "                  [\"PROJECT_PLANNINGSTAGE\",  \"j.find('PlanningStage').text\",                   \"STRING\"  ],\n",
    "                  [\"PROJECT_INRESEARCH\",     \"j.find('InResearch').text\",                      \"INTEGER\"       ],                       \n",
    "                  [\"PROJECT_FINALUSE\",       \"j.find('ProjFinalUseId').text\",                  \"STRING\"  ],\n",
    "                  [\"PROJECT_VALUE\",          \"j.find('ProjValue').text\",                       \"FLOAT\"       ],\n",
    "                  [\"PROJECT_SIZE\",           \"j.find('ProjSize').text\",                        \"FLOAT\"       ],\n",
    "                  [\"PROJECT_VOLUME\",         \"j.find('ProjVolume').text\",                      \"FLOAT\"       ],\n",
    "                  [\"PROJECT_APARTMENTS\",     \"j.find('ProjApartments').text\",                  \"FLOAT\"       ],\n",
    "                  [\"PROJECT_FLOORS\",         \"j.find('ProjFloors').text\",                      \"FLOAT\"       ],\n",
    "                  [\"PROJECT_BUILDINGS\",      \"j.find('ProjBuildings').text\",                   \"FLOAT\"       ],                 \n",
    "                  [\"PROJECT_NOTE\",           \"j.find('ProjNote').text\",                        \"STRING\" ],\n",
    "                  [\"DATE_INSERTION\",         \"j.find('ProjInsertDate').text[:19]\",             \"timestamp\"    ],\n",
    "                  [\"DATE_UPDATE\",            \"j.find('ProjLatestUpdate').text[:19]\",           \"timestamp\"    ],\n",
    "                  [\"DATE_PUBLICATION\",       \"j.find('ProjConfirmDate').text[:19]\",            \"timestamp\"    ],\n",
    "                  [\"DATE_STARTCONSTRUCTION\", \"j.find('ProjConstStartDate').text[:19]\",         \"timestamp\"    ],\n",
    "                  [\"DATE_PLANNINGAPPROVAL\",  \"j.find('ProjPlanningApprovalDate').text[:19]\",   \"timestamp\"    ],\n",
    "                  [\"DETAIL_ROOFTYPE\",        \"j.find('RoofType').text\",                        \"STRING\"  ],\n",
    "                  [\"DETAIL_MATERIALS\",       \"j.find('ConstructionMaterials').text\",           \"STRING\"  ],\n",
    "                  [\"DETAIL_CLADDING\",        \"j.find('CladdingType').text\",                    \"STRING\"  ],\n",
    "                  [\"DETAIL_HEATING\",         \"j.find('HeatingType').text\",                     \"STRING\"  ], \n",
    "                  [\"DETAIL_SOLAR\",           \"j.find('ProjSolar').text\",                       \"INTEGER\"       ], \n",
    "                  [\"ADDRESS_STREET1\",        \"j.find('ProjAddress1').text\",                    \"STRING\"  ],                                     \n",
    "                  [\"ADDRESS_STREET2\",        \"j.find('ProjAddress2').text\",                    \"STRING\"  ], \n",
    "                  [\"ADDRESS_STREET3\",        \"j.find('ProjAddress3').text\",                    \"STRING\"  ],                   \n",
    "                  [\"ADDRESS_CITY\",           \"j.find('ProjCity').text\",                        \"STRING\"  ],\n",
    "                  [\"ADDRESS_POSTALCODE\",     \"j.find('ProjZip').text\",                         \"FLOAT\"       ],\n",
    "                  [\"ADDRESS_COUNTY\",         \"j.find('ProjCounty').text\",                      \"STRING\"  ],\n",
    "                  [\"ADDRESS_CANTON\",         \"canton_dict[j.find('ProjRegId').text]\",          \"STRING\"   ], \n",
    "                  [\"ADDRESS_COUNTRY\",        \"j.find('ProjCountryId').text\",                   \"STRING\"   ]]\n",
    "\n",
    "building_list  = [[\"PROJECT_ID\",             \"l.find('PProProjId').text\",                      \"STRING\"  ],\n",
    "                  [\"BUILDING_TYPE\",          \"l.find('BuildingType').text\",                    \"STRING\"  ],    \n",
    "                  [\"BUILDING_DEVELOPMENT\",   \"l.find('DevelopmentType').text\",                 \"STRING\"  ]]\n",
    "\n",
    "contact_list   = [[\"PROJECT_ID\",             \"n.find('TarProjId').text\",                       \"STRING\"  ],  \n",
    "                  [\"ORG_TYPE\",               \"n.find('TargetGroupType').text\",                 \"STRING\"  ],   \n",
    "                  [\"ORG_ID\",                 \"n.find('OrgId').text\",                           \"STRING\"  ], \n",
    "                  [\"ORG_NAME\",               \"n.find('OrgName').text\",                         \"STRING\" ], \n",
    "                  [\"ORG_STREET1\",            \"n.find('OrgAddress1').text\",                     \"STRING\"  ],                                     \n",
    "                  [\"ORG_STREET2\",            \"n.find('OrgAddress2').text\",                     \"STRING\"  ], \n",
    "                  [\"ORG_STREET3\",            \"n.find('OrgAddress3').text\",                     \"STRING\"  ],                   \n",
    "                  [\"ORG_CITY\",               \"n.find('OrgCity').text\",                         \"STRING\"  ],\n",
    "                  [\"ORG_COUNTRY\",            \"n.find('OrgCountryId').text\",                    \"STRING\"   ],\n",
    "                  [\"ORG_POSTALCODE\",         \"n.find('OrgZip').text\",                          \"STRING\"   ],\n",
    "                  [\"ORG_PB_ADDRESS\",         \"n.find('OrgPostBoxAddress').text\",               \"STRING\"  ],                                     \n",
    "                  [\"ORG_PB_CITY\",            \"n.find('OrgPostBoxCity').text\",                  \"STRING\"  ], \n",
    "                  [\"ORG_PB_POSTALCODE\",      \"n.find('OrgPostBoxZip').text\",                   \"STRING\"   ],                   \n",
    "                  [\"ORG_PHONE\",              \"n.find('OrgPhone').text\",                        \"STRING\"  ],\n",
    "                  [\"ORG_EMAIL\",              \"n.find('OrgEmail').text\",                        \"STRING\"  ],\n",
    "                  [\"ORG_WEB\",                \"n.find('OrgWeb').text\",                          \"STRING\"  ],\n",
    "                  [\"ORG_EMPLOYEES\",          \"n.find('Employees').text\",                       \"STRING\"  ],\n",
    "                  [\"PERSON_ID\",              \"n.find('PerId').text\",                           \"STRING\"  ], \n",
    "                  [\"PERSON_GENDER\",          \"n.find('PerGender').text\",                       \"STRING\"   ],                                     \n",
    "                  [\"PERSON_FIRSTNAME\",       \"n.find('PerFname').text\",                        \"STRING\"  ], \n",
    "                  [\"PERSON_LASTNAME\",        \"n.find('PerLname').text\",                        \"STRING\"  ],                   \n",
    "                  [\"PERSON_PHONE\",           \"n.find('PerPhone').text\",                        \"STRING\"  ],\n",
    "                  [\"PERSON_MOBILE\",          \"n.find('PerMobile').text\",                       \"STRING\"  ],\n",
    "                  [\"PERSON_EMAIL\",           \"n.find('PerEmail').text\",                        \"STRING\"  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "#1.5 Feature Exctraction\n",
    "print(\"1.5\")\n",
    "\n",
    "def df_generator(attr_list):\n",
    "    df = pd.DataFrame(columns = [i[0] for i in attr_list])\n",
    "    for i in attr_list:\n",
    "        df[i[0]] = df[i[0]].astype(object)\n",
    "    return df\n",
    "\n",
    "df_projects = df_generator(project_list) \n",
    "df_buildings = df_generator(building_list) \n",
    "df_contacts = df_generator(contact_list) \n",
    "i_building = i_contact = 0\n",
    "\n",
    "for i, j in enumerate(root.find(\"Projects\")):\n",
    "\n",
    "    #Feature Extraction df_projects\n",
    "    for k in project_list:\n",
    "        try: df_projects.at[i, k[0]] = eval(k[1])\n",
    "        except AttributeError: df_projects.at[i, k[0]] = np.nan\n",
    "\n",
    "    #Feature Extraction df_buildings    \n",
    "    for l in j.findall(\"BuildingType\"):\n",
    "        for k in building_list:\n",
    "            try: df_buildings.at[i_building, k[0]] = eval(k[1])\n",
    "            except AttributeError: df_buildings.at[i_building, k[0]] = np.nan\n",
    "        i_building += 1\n",
    "\n",
    "    #Feature Extraction df_target    \n",
    "    for n in j.findall(\"TargetGroup\"):\n",
    "        for k in contact_list:\n",
    "            try: df_contacts.at[i_contact, k[0]] = eval(k[1])\n",
    "            except AttributeError: df_contacts.at[i_contact, k[0]] = np.nan    \n",
    "        i_contact += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6\n"
     ]
    }
   ],
   "source": [
    "#1.6 Data Preparation\n",
    "#df = df.apply(lambda col: col.apply(lambda x: int(x) if pd.notnull(x) else x), axis=1)\n",
    "#fillna(0)\n",
    "print(\"1.6\")\n",
    "#General Data Preparation\n",
    "def data_cleanser(df, attr_list):    \n",
    "    for i in attr_list: \n",
    "        if i[0] in [\"PROJECT_INRESEARCH\", \"DETAIL_SOLAR\"]: df[i[0]] = df[i[0]].apply(lambda x: 1 if x == \"True\" else 0)\n",
    "        elif i[2]     == \"timestamp\": df[i[0]] = df[i[0]].apply(lambda x: pd.to_datetime(x, format = \"%Y-%m-%dT%H:%M:%S\")).fillna(datetime.datetime(1900,1,1))\n",
    "        elif i[2]     == \"FLOAT\":     df[i[0]] = df[i[0]].astype(float).fillna(0)\n",
    "        elif i[2]     == \"STRING\":    df[i[0]] = df[i[0]].apply(lambda x: x if type(x) != str else x.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")).fillna(\"\")\n",
    "    return df\n",
    "\n",
    "df_projects  = data_cleanser(df_projects,  project_list)\n",
    "df_buildings = data_cleanser(df_buildings, building_list)\n",
    "df_contacts  = data_cleanser(df_contacts,  contact_list)\n",
    "\n",
    "#Cleaning df_projects\n",
    "df_projects[\"PROJECT_FINALUSE\"] = df_projects.PROJECT_FINALUSE.map(description_dict['ProjectFinalUse'])\n",
    "df_projects[\"PROJECT_FINALUSE\"] = df_projects.PROJECT_FINALUSE.fillna(\"\")\n",
    "df_projects[\"PROJECT_VALUE\"] = df_projects.PROJECT_VALUE.apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "#Cleaning df_contacts\n",
    "def phone_number_sanitizer(x):\n",
    "    if x == None or x==\"\":                     return None\n",
    "    else: x = x.replace(\" \", \"\")\n",
    "\n",
    "    if any(c.isalpha() for c in x):     return None\n",
    "    elif (x[0] == \"0\") & (x[1] != \"0\"): return \"+41\" + x[1:]\n",
    "    elif x[:2] == \"00\":                 return \"+\" + x[2:]\n",
    "    return \n",
    "\n",
    "df_contacts[\"ORG_PHONE\"]     = df_contacts.ORG_PHONE.apply(phone_number_sanitizer).fillna(\"\")\n",
    "df_contacts[\"PERSON_PHONE\"]  = df_contacts.PERSON_PHONE.apply(phone_number_sanitizer).fillna(\"\")\n",
    "df_contacts[\"PERSON_MOBILE\"] = df_contacts.PERSON_MOBILE.apply(phone_number_sanitizer).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_projects.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1\n",
      "2.2\n"
     ]
    }
   ],
   "source": [
    "#2   Update Database\n",
    "#https://stackoverflow.com/questions/51708355/bigquery-standard-sql-not-deleted/51710920\n",
    "#https://stackoverflow.com/questions/31652001/big-query-update-or-delete-issue\n",
    "\n",
    "if (stage == \"DEV\") & (df_projects.shape[0]>0):\n",
    "\n",
    "    dataset_id = 'BINDEXIS'\n",
    "\n",
    "    load_config_projects = bigquery.LoadJobConfig()\n",
    "    load_config_projects.schema = [bigquery.SchemaField(i[0], i[2]) for i in project_list]\n",
    "    load_config_buildings = bigquery.LoadJobConfig()\n",
    "    load_config_buildings.schema = [bigquery.SchemaField(i[0], i[2]) for i in building_list]\n",
    "    load_config_contacts = bigquery.LoadJobConfig()\n",
    "    load_config_contacts.schema = [bigquery.SchemaField(i[0], i[2]) for i in contact_list]\n",
    "\n",
    "    client_bq = bigquery.Client()\n",
    "    dataset_ref = client_bq.dataset(dataset_id)\n",
    "    table_ref_projects = dataset_ref.table('bindexis_bau_projects')\n",
    "    table_ref_buildings = dataset_ref.table('bindexis_bau_buildings')\n",
    "    table_ref_contacts = dataset_ref.table('bindexis_bau_contacts')\n",
    "    \n",
    "    #2.1 Delete Records\n",
    "    print(\"2.1\")\n",
    "    for i in [\"projects\", \"buildings\", \"contacts\"]:\n",
    "        if i==\"projects\":\n",
    "            tmp_del = list(df_projects[\"PROJECT_ID\"].unique())\n",
    "        elif i==\"buildings\":\n",
    "            tmp_del = list(df_buildings[\"PROJECT_ID\"].unique())\n",
    "        elif i==\"contacts\":\n",
    "            tmp_del = list(df_contacts[\"PROJECT_ID\"].unique())\n",
    "\n",
    "        client_bq.query(\"\"\"DELETE FROM {0}.bindexis_bau_{1} where PROJECT_ID IN \n",
    "                (SELECT *\n",
    "                FROM UNNEST({2})\n",
    "                  AS PROJECT_ID)\"\"\".format(dataset_id,i,tmp_del)).result()\n",
    "\n",
    "    #2.2 Insert Updated Records\n",
    "    print(\"2.2\")    \n",
    "    client_bq.load_table_from_dataframe(df_projects, table_ref_projects, project=PROJECT, job_config=load_config_projects).result()\n",
    "    client_bq.load_table_from_dataframe(df_buildings, table_ref_buildings, project=PROJECT, job_config=load_config_buildings).result()\n",
    "    client_bq.load_table_from_dataframe(df_contacts, table_ref_contacts, project=PROJECT, job_config=load_config_contacts).result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2\n"
     ]
    }
   ],
   "source": [
    "#3.2 Speicherung Parameter & Backup bei erfolgreichem Lauf\n",
    "print(\"3.2\")\n",
    "if stage == \"DEV\": \n",
    "    filename = \"{}/Parameter_TimeLastRun.pkl\".format(tempfile.gettempdir())\n",
    "    with open(filename, 'wb') as fp: pickle.dump(time_now, fp)\n",
    "    blob = bucket_cs.blob(path_data_va+'Parameter_TimeLastRun.pkl')\n",
    "    blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table BINDEXIS.bindexis_bau_projects created new\n",
      "Table BINDEXIS.bindexis_bau_buildings created new\n",
      "Table BINDEXIS.bindexis_bau_contacts created new\n"
     ]
    }
   ],
   "source": [
    "#4   Auxiliary Functions\n",
    "#4.2 Reset Bigquery Tables\n",
    "# https://cloud.google.com/bigquery/docs/tables\n",
    "from google.cloud import bigquery\n",
    "client_bq = bigquery.Client()\n",
    "dataset_id = 'BINDEXIS'\n",
    "\n",
    "def aux_reset_bigquery(list_name, table_name):\n",
    "    try:\n",
    "        table_ref = client_bq.dataset(dataset_id).table(table_name)\n",
    "        client_bq.delete_table(table_ref)  # API request\n",
    "        print(\"Table {}.{} deleted and created new\".format(dataset_id,table_name))\n",
    "    except:\n",
    "        print(\"Table {}.{} created new\".format(dataset_id,table_name))\n",
    "    \n",
    "    schema = [bigquery.SchemaField(i[0], i[2]) for i in list_name]\n",
    "    table_ref = client_bq.dataset(dataset_id).table(table_name)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client_bq.create_table(table)  # API request\n",
    "\n",
    "    assert table.table_id == table_name\n",
    "    return\n",
    "   \n",
    "aux_reset_bigquery(project_list, \"bindexis_bau_projects\")\n",
    "aux_reset_bigquery(building_list,  \"bindexis_bau_buildings\")    \n",
    "aux_reset_bigquery(contact_list, \"bindexis_bau_contacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%bq` not found.\n"
     ]
    }
   ],
   "source": [
    "%%bq \n",
    "tables describe --name \\\"axa-ch-datalake-analytics-dev'.BINDEXIS.bindexis_bau_projects\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "requests>=2.21.0\n",
    "numpy>=1.16.3\n",
    "pandas>=0.24.2\n",
    "google-cloud-storage>=1.15.0\n",
    "google-cloud-bigquery>=1.11.2\n",
    "pyarrow>=0.13.0\n",
    "fastparquet>=0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://requirements.txt [Content-Type=text/plain]...\n",
      "/ [1 files][  140.0 B/  140.0 B]                                                \n",
      "Operation completed over 1 objects/140.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp requirements.txt gs://${BUCKET}/bindexis/data/various"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with NaN, NaT, ... to insert in BigQuery\n",
    "None at object (string) works\n",
    " \n",
    "RequestException: HTTP request failed: Invalid JSON payload received. Unexpected token.\n",
    "n\", \"PERSON_PHONE\": NaN, \"PROJECT_ID\": \"\n",
    "                    ^\n",
    "                    \n",
    "Datatypes dataframe to datatypes bigquery\n",
    "Pandas-Float to BG-Integer\n",
    "\n",
    "to_gbq doesn't work -> VErsion conflict???\n",
    "TypeError: to_gbq() got an unexpected keyword argument 'table_schema'\n",
    "\n",
    "Write a pickle (last_run) to gcp doesn't work\n",
    "\n",
    "df column empty as object. but load to BQ doesn't work. Wants to import it as integer ?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Speicherung Parameter & Backup bei erfolgreichem Lauf\n",
    "print(\"3.2\")\n",
    "if stage == \"DEV\": \n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET)\n",
    "    blob = bucket.blob(path_data_va+'Parameter_TimeLastRun.txt')\n",
    "    blob.upload_from_string(str(time_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4   Auxiliary Functions\n",
    "\n",
    "#4.1 Create Dataset\n",
    "# https://cloud.google.com/bigquery/docs/datasets\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "dataset_id = 'BINDEXIS'\n",
    "\n",
    "# Create a DatasetReference using a chosen dataset ID.\n",
    "# The project defaults to the Client's project if not specified.\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "# Specify the geographic location where the dataset should reside.\n",
    "dataset.location = \"europe-west6\"\n",
    "\n",
    "# Send the dataset to the API for creation.\n",
    "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# exists within the project.\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset)  # API request\n",
    "except:\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4   Auxiliary Functions\n",
    "import google.datalab.bigquery as bg\n",
    "\n",
    "#4.1 Create the dataset\n",
    "dataset_id = 'BINDEXIS'\n",
    "try:\n",
    "  bq.Dataset(dataset_id).create()\n",
    "except:\n",
    "  print(\"dataset already exists\")\n",
    "  \n",
    "#4.2 Reset Oracle Tables\n",
    "def aux_reset_oracle(list_name, table_name):\n",
    "    try:\n",
    "        bq.Query(\"drop table {}.{}\".format(dataset_id,table_name)).execute()\n",
    "        print(\"Table {}.{} deleted and created new\".format(dataset_id,table_name))\n",
    "    except:\n",
    "        print(\"Table {}.{} created new\".format(dataset_id,table_name))\n",
    "    schema = [{\"name\": i[0] , \"type\": i[2]} for i in list_name]\n",
    "    #schema = bq.Schema.from_data(dataframe)\n",
    "    table = bq.Table(\n",
    "        '{}.{}'.format(dataset_id,table_name)).create(schema=schema)\n",
    "    return\n",
    "   \n",
    "aux_reset_oracle(project_list, \"bindexis_bau_projects\")\n",
    "#aux_reset_oracle(building_list,  \"bindexis_bau_buildings\")    \n",
    "#aux_reset_oracle(contact_list, \"bindexis_bau_contacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1   Retrieve Bindexis Data\n",
    "#1.1 Access & Authentication Setup\n",
    "print(\"1.1\")\n",
    "username      = \"TIppisch\"\n",
    "time_now      = datetime.datetime.now()\n",
    "#with open(path_data + \"Parameter_TimeLastRun.pkl\", 'rb') as fp: time_last_run = pickle.load(fp)\n",
    "time_last_run      = datetime.datetime.now() + datetime.timedelta(days=-4)\n",
    "print(time_last_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'gs://{0}/bindexis/data'.format(os.environ['BUCKET'])\n",
    "time_now      = datetime.datetime.now() + datetime.timedelta(days=-4)\n",
    "print(time_now)\n",
    "with gcs.(path_data + \"Parameter_TimeLastRun.pkl\", 'wb') as fp: pickle.dump(time_now, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "\n",
    "bq.Query(\"\"\"DELETE FROM {}.{} where PROJECT_ID IN \n",
    "        (SELECT *\n",
    "        FROM UNNEST({})\n",
    "          AS PROJECT_ID)\"\"\".format(\"BINDEXIS\",\"bindexis_bau_buildings\",a)).execute()\n",
    "\n",
    "\n",
    "# In[16]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSchema",
     "evalue": "Please verify that the structure and data types in the DataFrame match the schema of the destination table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6ef49ca4e604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import pandas_gbq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproject_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_projects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gbq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BINDEXIS.bindexis_bau_projects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(self, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key)\u001b[0m\n\u001b[1;32m   1426\u001b[0m             \u001b[0mtable_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m             verbose=verbose, private_key=private_key)\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mauth_local_webserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth_local_webserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         credentials=credentials, verbose=verbose, private_key=private_key)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key)\u001b[0m\n\u001b[1;32m   1139\u001b[0m             ):\n\u001b[1;32m   1140\u001b[0m                 raise InvalidSchema(\n\u001b[0;32m-> 1141\u001b[0;31m                     \u001b[0;34m\"Please verify that the structure and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                     \u001b[0;34m\"data types in the DataFrame match the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0;34m\"schema of the destination table.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidSchema\u001b[0m: Please verify that the structure and data types in the DataFrame match the schema of the destination table."
     ]
    }
   ],
   "source": [
    "#import pandas_gbq\n",
    "schema = [{\"name\": i[0] , \"type\": i[2]} for i in project_list]\n",
    "df_projects.to_gbq(destination_table=\"BINDEXIS.bindexis_bau_projects\", project_id=PROJECT, if_exists='append', table_schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_bq_schema(df, default_type='STRING'):\n",
    "    \"\"\" Given a passed df, generate the associated Google BigQuery schema.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    default_type : string\n",
    "        The default big query type in case the type of the column\n",
    "        does not exist in the schema.\n",
    "    \"\"\"\n",
    "\n",
    "    type_mapping = {\n",
    "        'i': 'INTEGER',\n",
    "        'b': 'BOOLEAN',\n",
    "        'f': 'FLOAT',\n",
    "        'O': 'STRING',\n",
    "        'S': 'STRING',\n",
    "        'U': 'STRING',\n",
    "        'M': 'TIMESTAMP'\n",
    "    }\n",
    "\n",
    "    fields = []\n",
    "    for column_name, dtype in df.dtypes.iteritems():\n",
    "        fields.append({'name': column_name,\n",
    "                       'type': type_mapping.get(dtype.kind, default_type)})\n",
    "\n",
    "    return {'fields': fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudstorage\n",
      "  Downloading https://files.pythonhosted.org/packages/33/e4/bdfeabdfec580c4ddb9b6f2cfc3e32b6767308d3cee0cb84a85cf63599f1/cloudstorage-0.9.0-py3-none-any.whl (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.7MB/s \n",
      "\u001b[?25hCollecting python-magic>=0.4.15 (from cloudstorage)\n",
      "  Downloading https://files.pythonhosted.org/packages/42/a1/76d30c79992e3750dac6790ce16f056f870d368ba142f83f75f694d93001/python_magic-0.4.15-py2.py3-none-any.whl\n",
      "Collecting inflection>=0.3.1 (from cloudstorage)\n",
      "  Downloading https://files.pythonhosted.org/packages/d5/35/a6eb45b4e2356fe688b21570864d4aa0d0a880ce387defe9c589112077f8/inflection-0.3.1.tar.gz\n",
      "Collecting python-dateutil>=2.7.3 (from cloudstorage)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7.3->cloudstorage)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: inflection\n",
      "  Running setup.py bdist_wheel for inflection ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/9f/5a/d3/6fc3bf6516d2a3eb7e18f9f28b472110b59325f3f258fe9211\n",
      "Successfully built inflection\n",
      "Installing collected packages: python-magic, inflection, six, python-dateutil, cloudstorage\n",
      "Successfully installed cloudstorage-0.9.0 inflection-0.3.1 python-dateutil-2.8.0 python-magic-0.4.15 six-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudstorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudstorage as gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-22 11:18:28.819143\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cloudstorage' has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3c1c7d3b43df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_now\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Parameter_TimeLastRun2.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_now\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cloudstorage' has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "import cloudstorage as gcs\n",
    "path_data = 'gs://{0}/bindexis/data'.format(os.environ['BUCKET'])\n",
    "time_now      = datetime.datetime.now() + datetime.timedelta(days=-4)\n",
    "print(time_now)\n",
    "with gcs.(path_data + \"Parameter_TimeLastRun2.pkl\", 'wb') as fp: pickle.dump(time_now, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.to_parquet('gs://axa-ch-raw-dev-dla/'+path_data_va+'bindexis_output.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.to_csv('gs://axa-ch-raw-dev-dla/'+path_data_va+'Parameter_TimeLastRun3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'datetime.datetime' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-42ec553b0991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_now\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://axa-ch-raw-dev-dla/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_data_va\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Parameter_TimeLastRun2.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'datetime.datetime' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "time_now.to_csv('gs://axa-ch-raw-dev-dla/'+path_data_va+'Parameter_TimeLastRun2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gs://axa-ch-raw-dev-dla/bindexis/data/various/Parameter_TimeLastRun2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-0d2ca76c69fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://axa-ch-raw-dev-dla/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_data_va\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Parameter_TimeLastRun2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mFileout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFileout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gs://axa-ch-raw-dev-dla/bindexis/data/various/Parameter_TimeLastRun2.txt'"
     ]
    }
   ],
   "source": [
    "with open('gs://axa-ch-raw-dev-dla/'+path_data_va+'Parameter_TimeLastRun2.txt',\"wb\") as csvFile:\n",
    "    Fileout = csv.writer(csvFile)\n",
    "    Fileout.writerow(time_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job be984446-5a0c-483c-a5d7-9b45bc13337f\n",
      "Job finished.\n",
      "Loaded 3610 rows.\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "dataset_id = 'BINDEXIS'\n",
    "table_ref = client.dataset(dataset_id).table('bindexis_bau_projects')\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "#job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "job_config.schema = [bigquery.SchemaField(i[0], i[2]) for i in project_list]\n",
    "uri = 'gs://axa-ch-raw-dev-dla/'+path_data_va+'output.parquet'\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_ref, job_config=job_config\n",
    ")  # API request\n",
    "print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "load_job.result()  # Waits for table load to complete.\n",
    "print(\"Job finished.\")\n",
    "\n",
    "destination_table = client.get_table(table_ref)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
