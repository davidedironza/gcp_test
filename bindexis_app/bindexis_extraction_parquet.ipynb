{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'axa-ch-raw-dev-dla'\n",
    "PROJECT = 'axa-ch-datalake-analytics-dev'\n",
    "REGION = 'eu-west6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0   Modul-Import & Parametrierung    \n",
    "#0.1 Modul-Import\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import tempfile\n",
    "import logging\n",
    "from html import unescape\n",
    "from random import randint\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2 Set Stage\n",
    "stage = \"DEV\" #sf.platform_is_server(\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3 Pfadinformationen\n",
    "path_data_va = \"bindexis/data/various/\"\n",
    "path_data_input = \"bindexis/data/input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.4 Set timezone\n",
    "os.environ['TZ'] = 'Europe/Zurich'\n",
    "time.tzset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "2019-05-06 07:43:24.262596+02:00\n"
     ]
    }
   ],
   "source": [
    "#1   Retrieve Bindexis Data\n",
    "#1.1 Access & Authentication Setup\n",
    "print(\"1.1\")\n",
    "username      = \"TIppisch\"\n",
    "time_now      = datetime.datetime.now(pytz.timezone('Europe/Zurich'))\n",
    "\n",
    "client_cs = storage.Client()\n",
    "bucket_cs = client_cs.get_bucket(BUCKET)\n",
    "try:\n",
    "    filename = \"{}/Parameter_TimeLastRun.pkl\".format(tempfile.gettempdir())\n",
    "    #blob = bucket_cs.blob(path_data_va+'Parameter_TimeLastRun.txt')\n",
    "    #time_last_run = blob.download_as_string()\n",
    "    #time_last_run = datetime.datetime.strptime(time_last_run.decode(\"utf-8\")[:-6], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    blob = bucket_cs.blob(path_data_va+'Parameter_TimeLastRun.pkl')\n",
    "    blob.download_to_filename(filename)\n",
    "    with open(filename, 'rb') as fp: time_last_run = pickle.load(fp)\n",
    "except:\n",
    "    time_last_run      = time_now + datetime.timedelta(days=-2)\n",
    "\n",
    "print(time_last_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"1.2\")\n",
    "#file = \"../certificates/sas_server_keystore.pem\"\n",
    "data = \"\"\"<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:soap=\"http://soap.bindexis.ch/\">\n",
    "           <soapenv:Header>\n",
    "              <soap:AuthHeader>\n",
    "                 <!--Optional:-->\n",
    "                 <soap:UserName>{0}</soap:UserName>\n",
    "                 <!--Optional:-->\n",
    "                 <soap:Password>{1}</soap:Password>\n",
    "              </soap:AuthHeader>\n",
    "           </soapenv:Header>\n",
    "           <soapenv:Body>\n",
    "              <soap:GetProjectSync>\n",
    "                 <!--Optional:-->\n",
    "                 <soap:dateToStart>{2}</soap:dateToStart>\n",
    "              </soap:GetProjectSync>\n",
    "           </soapenv:Body>\n",
    "        </soapenv:Envelope>\"\"\".format(username, \"19e22172\", time_last_run.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "response = requests.post(url     = \"https://soap.bindexis.ch/projectsync.asmx\",\n",
    "                         headers = {'content-type': 'text/xml'},\n",
    "                         #cert    = file,\n",
    "                         data    = data,\n",
    "                         verify  = False)\n",
    "\n",
    "base_text = unescape(response.text)\n",
    "root = ET.fromstring(base_text[base_text.find(\"<?xml\", 1): base_text.find(\"</GetProjectSyncResult>\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.21 Save response.text to GCS\n",
    "blob = bucket_cs.blob(path_data_input+'bindexis_{}.txt'.format(time_now.strftime('%Y%m%d%H%M%S')))\n",
    "blob.upload_from_string(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n"
     ]
    }
   ],
   "source": [
    "#1.3 Extraction Description Dictionary\n",
    "print(\"1.3\")\n",
    "description_dict = {}\n",
    "\n",
    "for i in [\"PlanningStage\", \"SizeUnit\", \"RoofType\", \"ConstructionMaterial\", \"ProjCladdingType\", \n",
    "          \"DevelopmentType\", \"TargetGroupType\", \"Employee\", \"JobFunction\", \"Gender\", \"BuildingType\", \n",
    "          \"HeatingType\", \"ProjectFinalUse\"]:\n",
    "\n",
    "    aux_dict = {}\n",
    "    if i == \"ProjectFinalUse\": values = root.find(\"{0}\".format(i))\n",
    "    else:                      values = root.find(\"{0}Values\".format(i))\n",
    "\n",
    "    if i == \"BuildingType\":\n",
    "        for j in values[1:]: aux_dict[j.find(\"ProId\").text] = [j.find(\"ProDesc\").text, j.find(\"ProParentId\").text]     \n",
    "    else: \n",
    "        for j in values: aux_dict[j.find(\"UtilId\").text] = j.find(\"UtilDesc\").text\n",
    "\n",
    "    description_dict[i] = aux_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4\n"
     ]
    }
   ],
   "source": [
    "#1.4 Definition of Extraction Configuration Files\n",
    "print(\"1.4\")\n",
    "canton_dict = {'CH01': 'AG',     'CH02': 'AI',     'CH03': 'AR',     'CH04': 'BE',     'CH05': 'BL', \n",
    "               'CH06': 'BS',     'CH08': 'FR',     'CH09': 'GE',     'CH10': 'GL',     'CH11': 'GR', \n",
    "               'CH12': 'JU',     'CH13': 'LU',     'CH14': 'NE',     'CH15': 'NW',     'CH16': 'OW',     \n",
    "               'CH17': 'SG',     'CH18': 'SH',     'CH19': 'SO',     'CH20': 'SZ',     'CH21': 'TG',             \n",
    "               'CH22': 'TI',     'CH23': 'UR',     'CH24': 'VD',     'CH25': 'VS',     'CH26': 'ZG',     \n",
    "               'CH27': 'ZH'}  \n",
    "\n",
    "project_list =   [[\"PROJECT_ID\",             \"j.find('ProjId').text\",                          \"STRING\"  ],\n",
    "                  [\"PROJECT_TITLE\",          \"j.find('ProjTitle').text\",                       \"STRING\"  ],   \n",
    "                  [\"PROJECT_PARCELID\",       \"j.find('ProjOfficialId').text\",                  \"STRING\"  ],  \n",
    "                  [\"PROJECT_LANGUAGE\",       \"j.find('ProjLanguageId').text[:2].upper()\",      \"STRING\"   ],\n",
    "                  [\"PROJECT_DESCRIPTION\",    \"j.find('ProjDesc').text\",                        \"STRING\"],\n",
    "                  [\"PROJECT_PLANNINGSTAGE\",  \"j.find('PlanningStage').text\",                   \"STRING\"  ],\n",
    "                  [\"PROJECT_INRESEARCH\",     \"j.find('InResearch').text\",                      \"INTEGER\"       ],                       \n",
    "                  [\"PROJECT_FINALUSE\",       \"j.find('ProjFinalUseId').text\",                  \"STRING\"  ],\n",
    "                  [\"PROJECT_VALUE\",          \"j.find('ProjValue').text\",                       \"FLOAT\"       ],\n",
    "                  [\"PROJECT_SIZE\",           \"j.find('ProjSize').text\",                        \"FLOAT\"       ],\n",
    "                  [\"PROJECT_VOLUME\",         \"j.find('ProjVolume').text\",                      \"FLOAT\"       ],\n",
    "                  [\"PROJECT_APARTMENTS\",     \"j.find('ProjApartments').text\",                  \"FLOAT\"       ],\n",
    "                  [\"PROJECT_FLOORS\",         \"j.find('ProjFloors').text\",                      \"FLOAT\"       ],\n",
    "                  [\"PROJECT_BUILDINGS\",      \"j.find('ProjBuildings').text\",                   \"FLOAT\"       ],                 \n",
    "                  [\"PROJECT_NOTE\",           \"j.find('ProjNote').text\",                        \"STRING\" ],\n",
    "                  [\"DATE_INSERTION\",         \"j.find('ProjInsertDate').text[:19]\",             \"timestamp\"    ],\n",
    "                  [\"DATE_UPDATE\",            \"j.find('ProjLatestUpdate').text[:19]\",           \"timestamp\"    ],\n",
    "                  [\"DATE_PUBLICATION\",       \"j.find('ProjConfirmDate').text[:19]\",            \"timestamp\"    ],\n",
    "                  [\"DATE_STARTCONSTRUCTION\", \"j.find('ProjConstStartDate').text[:19]\",         \"timestamp\"    ],\n",
    "                  [\"DATE_PLANNINGAPPROVAL\",  \"j.find('ProjPlanningApprovalDate').text[:19]\",   \"timestamp\"    ],\n",
    "                  [\"DETAIL_ROOFTYPE\",        \"j.find('RoofType').text\",                        \"STRING\"  ],\n",
    "                  [\"DETAIL_MATERIALS\",       \"j.find('ConstructionMaterials').text\",           \"STRING\"  ],\n",
    "                  [\"DETAIL_CLADDING\",        \"j.find('CladdingType').text\",                    \"STRING\"  ],\n",
    "                  [\"DETAIL_HEATING\",         \"j.find('HeatingType').text\",                     \"STRING\"  ], \n",
    "                  [\"DETAIL_SOLAR\",           \"j.find('ProjSolar').text\",                       \"INTEGER\"       ], \n",
    "                  [\"ADDRESS_STREET1\",        \"j.find('ProjAddress1').text\",                    \"STRING\"  ],                                     \n",
    "                  [\"ADDRESS_STREET2\",        \"j.find('ProjAddress2').text\",                    \"STRING\"  ], \n",
    "                  [\"ADDRESS_STREET3\",        \"j.find('ProjAddress3').text\",                    \"STRING\"  ],                   \n",
    "                  [\"ADDRESS_CITY\",           \"j.find('ProjCity').text\",                        \"STRING\"  ],\n",
    "                  [\"ADDRESS_POSTALCODE\",     \"j.find('ProjZip').text\",                         \"FLOAT\"       ],\n",
    "                  [\"ADDRESS_COUNTY\",         \"j.find('ProjCounty').text\",                      \"STRING\"  ],\n",
    "                  [\"ADDRESS_CANTON\",         \"canton_dict[j.find('ProjRegId').text]\",          \"STRING\"   ], \n",
    "                  [\"ADDRESS_COUNTRY\",        \"j.find('ProjCountryId').text\",                   \"STRING\"   ]]\n",
    "\n",
    "building_list  = [[\"PROJECT_ID\",             \"l.find('PProProjId').text\",                      \"STRING\"  ],\n",
    "                  [\"BUILDING_TYPE\",          \"l.find('BuildingType').text\",                    \"STRING\"  ],    \n",
    "                  [\"BUILDING_DEVELOPMENT\",   \"l.find('DevelopmentType').text\",                 \"STRING\"  ]]\n",
    "\n",
    "contact_list   = [[\"PROJECT_ID\",             \"n.find('TarProjId').text\",                       \"STRING\"  ],  \n",
    "                  [\"ORG_TYPE\",               \"n.find('TargetGroupType').text\",                 \"STRING\"  ],   \n",
    "                  [\"ORG_ID\",                 \"n.find('OrgId').text\",                           \"STRING\"  ], \n",
    "                  [\"ORG_NAME\",               \"n.find('OrgName').text\",                         \"STRING\" ], \n",
    "                  [\"ORG_STREET1\",            \"n.find('OrgAddress1').text\",                     \"STRING\"  ],                                     \n",
    "                  [\"ORG_STREET2\",            \"n.find('OrgAddress2').text\",                     \"STRING\"  ], \n",
    "                  [\"ORG_STREET3\",            \"n.find('OrgAddress3').text\",                     \"STRING\"  ],                   \n",
    "                  [\"ORG_CITY\",               \"n.find('OrgCity').text\",                         \"STRING\"  ],\n",
    "                  [\"ORG_COUNTRY\",            \"n.find('OrgCountryId').text\",                    \"STRING\"   ],\n",
    "                  [\"ORG_POSTALCODE\",         \"n.find('OrgZip').text\",                          \"STRING\"   ],\n",
    "                  [\"ORG_PB_ADDRESS\",         \"n.find('OrgPostBoxAddress').text\",               \"STRING\"  ],                                     \n",
    "                  [\"ORG_PB_CITY\",            \"n.find('OrgPostBoxCity').text\",                  \"STRING\"  ], \n",
    "                  [\"ORG_PB_POSTALCODE\",      \"n.find('OrgPostBoxZip').text\",                   \"STRING\"   ],                   \n",
    "                  [\"ORG_PHONE\",              \"n.find('OrgPhone').text\",                        \"STRING\"  ],\n",
    "                  [\"ORG_EMAIL\",              \"n.find('OrgEmail').text\",                        \"STRING\"  ],\n",
    "                  [\"ORG_WEB\",                \"n.find('OrgWeb').text\",                          \"STRING\"  ],\n",
    "                  [\"ORG_EMPLOYEES\",          \"n.find('Employees').text\",                       \"STRING\"  ],\n",
    "                  [\"PERSON_ID\",              \"n.find('PerId').text\",                           \"STRING\"  ], \n",
    "                  [\"PERSON_GENDER\",          \"n.find('PerGender').text\",                       \"STRING\"   ],                                     \n",
    "                  [\"PERSON_FIRSTNAME\",       \"n.find('PerFname').text\",                        \"STRING\"  ], \n",
    "                  [\"PERSON_LASTNAME\",        \"n.find('PerLname').text\",                        \"STRING\"  ],                   \n",
    "                  [\"PERSON_PHONE\",           \"n.find('PerPhone').text\",                        \"STRING\"  ],\n",
    "                  [\"PERSON_MOBILE\",          \"n.find('PerMobile').text\",                       \"STRING\"  ],\n",
    "                  [\"PERSON_EMAIL\",           \"n.find('PerEmail').text\",                        \"STRING\"  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "#1.5 Feature Exctraction\n",
    "print(\"1.5\")\n",
    "\n",
    "def df_generator(attr_list):\n",
    "    df = pd.DataFrame(columns = [i[0] for i in attr_list])\n",
    "    for i in attr_list:\n",
    "        df[i[0]] = df[i[0]].astype(object)\n",
    "    return df\n",
    "\n",
    "df_projects = df_generator(project_list) \n",
    "df_buildings = df_generator(building_list) \n",
    "df_contacts = df_generator(contact_list) \n",
    "i_building = i_contact = 0\n",
    "\n",
    "for i, j in enumerate(root.find(\"Projects\")):\n",
    "\n",
    "    #Feature Extraction df_projects\n",
    "    for k in project_list:\n",
    "        try: df_projects.at[i, k[0]] = eval(k[1])\n",
    "        except AttributeError: df_projects.at[i, k[0]] = np.nan\n",
    "\n",
    "    #Feature Extraction df_buildings    \n",
    "    for l in j.findall(\"BuildingType\"):\n",
    "        for k in building_list:\n",
    "            try: df_buildings.at[i_building, k[0]] = eval(k[1])\n",
    "            except AttributeError: df_buildings.at[i_building, k[0]] = np.nan\n",
    "        i_building += 1\n",
    "\n",
    "    #Feature Extraction df_target    \n",
    "    for n in j.findall(\"TargetGroup\"):\n",
    "        for k in contact_list:\n",
    "            try: df_contacts.at[i_contact, k[0]] = eval(k[1])\n",
    "            except AttributeError: df_contacts.at[i_contact, k[0]] = np.nan    \n",
    "        i_contact += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6\n"
     ]
    }
   ],
   "source": [
    "#1.6 Data Preparation\n",
    "#df = df.apply(lambda col: col.apply(lambda x: int(x) if pd.notnull(x) else x), axis=1)\n",
    "#fillna(0)\n",
    "print(\"1.6\")\n",
    "#General Data Preparation\n",
    "def data_cleanser(df, attr_list):    \n",
    "    for i in attr_list: \n",
    "        if i[0] in [\"PROJECT_INRESEARCH\", \"DETAIL_SOLAR\"]: df[i[0]] = df[i[0]].apply(lambda x: 1 if x == \"True\" else 0)\n",
    "        elif i[2]     == \"timestamp\": df[i[0]] = df[i[0]].apply(lambda x: pd.to_datetime(x, format = \"%Y-%m-%dT%H:%M:%S\")).fillna(datetime.datetime(1900,1,1))\n",
    "        elif i[2]     == \"FLOAT\":     df[i[0]] = df[i[0]].astype(float).fillna(0)\n",
    "        elif i[2]     == \"STRING\":    df[i[0]] = df[i[0]].apply(lambda x: x if type(x) != str else x.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")).fillna(\"\")\n",
    "    return df\n",
    "\n",
    "df_projects  = data_cleanser(df_projects,  project_list)\n",
    "df_buildings = data_cleanser(df_buildings, building_list)\n",
    "df_contacts  = data_cleanser(df_contacts,  contact_list)\n",
    "\n",
    "#Cleaning df_projects\n",
    "df_projects[\"PROJECT_FINALUSE\"] = df_projects.PROJECT_FINALUSE.map(description_dict['ProjectFinalUse'])\n",
    "df_projects[\"PROJECT_FINALUSE\"] = df_projects.PROJECT_FINALUSE.fillna(\"\")\n",
    "df_projects[\"PROJECT_VALUE\"] = df_projects.PROJECT_VALUE.apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "#Cleaning df_contacts\n",
    "def phone_number_sanitizer(x):\n",
    "    if x == None or x==\"\":                     return None\n",
    "    else: x = x.replace(\" \", \"\")\n",
    "\n",
    "    if any(c.isalpha() for c in x):     return None\n",
    "    elif (x[0] == \"0\") & (x[1] != \"0\"): return \"+41\" + x[1:]\n",
    "    elif x[:2] == \"00\":                 return \"+\" + x[2:]\n",
    "    return \n",
    "\n",
    "df_contacts[\"ORG_PHONE\"]     = df_contacts.ORG_PHONE.apply(phone_number_sanitizer).fillna(\"\")\n",
    "df_contacts[\"PERSON_PHONE\"]  = df_contacts.PERSON_PHONE.apply(phone_number_sanitizer).fillna(\"\")\n",
    "df_contacts[\"PERSON_MOBILE\"] = df_contacts.PERSON_MOBILE.apply(phone_number_sanitizer).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.7 Write temporary parquet-files\n",
    "df_projects.to_parquet('gs://{}/{}bindexis_projects_output.parquet'.format(BUCKET,path_data_va), index=False)\n",
    "df_buildings.to_parquet('gs://{}/{}bindexis_buildings_output.parquet'.format(BUCKET,path_data_va), index=False)\n",
    "df_contacts.to_parquet('gs://{}/{}bindexis_contacts_output.parquet'.format(BUCKET,path_data_va), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1\n",
      "2.2\n",
      "projects: Starting job 70764b23-672c-463d-8de4-59bbfd8ef896\n",
      "projects: Job finished.\n",
      "projects: Loaded 2547 rows.\n",
      "buildings: Starting job 248305ef-b6d2-49a9-9270-b059a0a73a1a\n",
      "buildings: Job finished.\n",
      "buildings: Loaded 4260 rows.\n",
      "contacts: Starting job 191982f8-6814-46d9-ba8d-42f74d5ede3f\n",
      "contacts: Job finished.\n",
      "contacts: Loaded 4681 rows.\n"
     ]
    }
   ],
   "source": [
    "#2   Update Database\n",
    "#https://stackoverflow.com/questions/51708355/bigquery-standard-sql-not-deleted/51710920\n",
    "#https://stackoverflow.com/questions/31652001/big-query-update-or-delete-issue\n",
    "\n",
    "if (stage == \"DEV\") & (df_projects.shape[0]>0):\n",
    "    client_bq = bigquery.Client()\n",
    "    dataset_id = 'BINDEXIS'\n",
    "    \n",
    "    #2.1 Delete Records\n",
    "    print(\"2.1\")\n",
    "    for i in [\"projects\", \"buildings\", \"contacts\"]:\n",
    "        if i==\"projects\":\n",
    "            tmp_del = list(df_projects[\"PROJECT_ID\"].unique())\n",
    "        elif i==\"buildings\":\n",
    "            tmp_del = list(df_buildings[\"PROJECT_ID\"].unique())\n",
    "        elif i==\"contacts\":\n",
    "            tmp_del = list(df_contacts[\"PROJECT_ID\"].unique())\n",
    "\n",
    "        client_bq.query(\"\"\"DELETE FROM {0}.bindexis_bau_{1} where PROJECT_ID IN \n",
    "                (SELECT *\n",
    "                FROM UNNEST({2})\n",
    "                  AS PROJECT_ID)\"\"\".format(dataset_id,i,tmp_del)).result()\n",
    "\n",
    "    #2.2 Insert Updated Records\n",
    "    def load_bigquery(list_name, table_name):\n",
    "        table_ref = client_bq.dataset(dataset_id).table('bindexis_bau_{}'.format(table_name))\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "        job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "        job_config.schema = [bigquery.SchemaField(i[0], i[2]) for i in list_name]\n",
    "        uri = 'gs://{}/{}bindexis_{}_output.parquet'.format(BUCKET,path_data_va,table_name)\n",
    "        load_job = client_bq.load_table_from_uri(uri, table_ref, job_config=job_config)  # API request\n",
    "        print(\"{}: Starting job {}\".format(table_name, load_job.job_id))\n",
    "\n",
    "        load_job.result()  # Waits for table load to complete.\n",
    "        print(\"{}: Job finished.\".format(table_name))\n",
    "\n",
    "        destination_table = client_bq.get_table(table_ref)\n",
    "        print(\"{}: Loaded {} rows.\".format(table_name, destination_table.num_rows))\n",
    "\n",
    "    print(\"2.2\")\n",
    "    load_bigquery(project_list, \"projects\")\n",
    "    load_bigquery(building_list, \"buildings\")\n",
    "    load_bigquery(contact_list, \"contacts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Speicherung Parameter & Backup bei erfolgreichem Lauf\n",
    "print(\"3.2\")\n",
    "if stage == \"DEV\": \n",
    "    filename = \"{}/Parameter_TimeLastRun.pkl\".format(tempfile.gettempdir())\n",
    "    with open(filename, 'wb') as fp: pickle.dump(time_now, fp)\n",
    "    blob = bucket_cs.blob(path_data_va+'Parameter_TimeLastRun.pkl')\n",
    "    blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table BINDEXIS.bindexis_bau_projects deleted and created new\n",
      "Table BINDEXIS.bindexis_bau_buildings deleted and created new\n",
      "Table BINDEXIS.bindexis_bau_contacts deleted and created new\n"
     ]
    }
   ],
   "source": [
    "#4   Auxiliary Functions\n",
    "#4.2 Reset Bigquery Tables\n",
    "# https://cloud.google.com/bigquery/docs/tables\n",
    "from google.cloud import bigquery\n",
    "client_bq = bigquery.Client()\n",
    "dataset_id = 'BINDEXIS'\n",
    "\n",
    "def aux_reset_bigquery(list_name, table_name):\n",
    "    try:\n",
    "        table_ref = client_bq.dataset(dataset_id).table(table_name)\n",
    "        client_bq.delete_table(table_ref)  # API request\n",
    "        print(\"Table {}.{} deleted and created new\".format(dataset_id,table_name))\n",
    "    except:\n",
    "        print(\"Table {}.{} created new\".format(dataset_id,table_name))\n",
    "    \n",
    "    schema = [bigquery.SchemaField(i[0], i[2]) for i in list_name]\n",
    "    table_ref = client_bq.dataset(dataset_id).table(table_name)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client_bq.create_table(table)  # API request\n",
    "\n",
    "    assert table.table_id == table_name\n",
    "    return\n",
    "   \n",
    "aux_reset_bigquery(project_list, \"bindexis_bau_projects\")\n",
    "aux_reset_bigquery(building_list,  \"bindexis_bau_buildings\")    \n",
    "aux_reset_bigquery(contact_list, \"bindexis_bau_contacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%bq` not found.\n"
     ]
    }
   ],
   "source": [
    "%%bq \n",
    "tables describe --name \\\"axa-ch-datalake-analytics-dev'.BINDEXIS.bindexis_bau_projects\\\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with NaN, NaT, ... to insert in BigQuery\n",
    "None at object (string) works\n",
    " \n",
    "RequestException: HTTP request failed: Invalid JSON payload received. Unexpected token.\n",
    "n\", \"PERSON_PHONE\": NaN, \"PROJECT_ID\": \"\n",
    "                    ^\n",
    "                    \n",
    "Datatypes dataframe to datatypes bigquery\n",
    "Pandas-Float to BG-Integer\n",
    "\n",
    "to_gbq doesn't work -> VErsion conflict???\n",
    "TypeError: to_gbq() got an unexpected keyword argument 'table_schema'\n",
    "\n",
    "Write a pickle (last_run) to gcp doesn't work\n",
    "\n",
    "df column empty as object. but load to BQ doesn't work. Wants to import it as integer ?!?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
